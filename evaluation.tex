\chapter{Evaluation}
\subsection{Segmentation metrics}

In \cite{pascalVoc2012},

Detections are considered true or false positives based on the area of overlap with ground truth bounding boxes. To be considered a correct detection, the area of overlap (also called Intersection ver Union) $a_o$ between the predicted bounding box $B_p$ and ground truth bounding box $B_{gt}$ must exceed 50\% by the formula:

$$a_o = \frac{area(B_p \cap B_{gt})}{area(B_p \cup B_{gt})} $$

To simplify the calculation, this formula can be rewritten as:

$$a_o = \frac{area(B_p \cap B_{gt})}{area(B_p) + area(B_{gt}) - area(B_p \cap B_{gt})} $$

\footnote{Information on the evaluation system can be found at  \url{http://host.robots.ox.ac.uk/pascal/VOC/voc2012/devkit_doc.pdf}}
% http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html
In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.

true positive = correctly classified
true negative = correctly unclassified
false positive = incorrectly classified
false negative = incorrectly classified

precision ($P$)  is defined as the number of true positives ($T_p$) over the number of true positives plus the number of false positives ($F_p$)./ recall / accuracy

$$ P =  \frac{T_p}{T_p + F_p}$$
$$ R =  \frac{T_p}{T_p + F_n}$$

Each segmentation competition will be judged by average segmentation accuracy across the twenty classes and the background class. The segmentation accuracy for a class will be assessed using the intersection/union metric, defined as the number of correctly labelled pixels of that class, divided by the number of pixels labelled with that class in either the ground truth labelling or the inferred labelling. Equivalently, the accuracy is given by the equation:

$$ A = \frac{T_p}{T_p + F_n + F_p} $$

\subsection{Classification metrics}

% http://scikit-learn.org/stable/modules/cross_validation.html
cross validation
accuracy
% http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion\_matrix.html
confusion matrix

\section{Results}