Automatically generated by Mendeley Desktop 1.16.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Martinel2015,
abstract = {Food recognition is an emerging computer vision topic. The problem is characterized by the absence of rigid struc-ture of the food and by the large intra-class variations. Ex-isting approaches tackle the problem by designing ad-hoc feature representations based on a priori knowledge of the problem. Differently from these, we propose a committee-based recognition system that chooses the optimal features out of the existing plethora of available ones (e.g., color, texture, etc.). Each committee member is an Extreme Learn-ing Machine trained to classify food plates on the basis of a single feature type. Single member classifications are then considered by a structural Support Vector Machine to produce the final ranking of possible matches. This is achieved by filtering out the irrelevant features/classifiers, thus considering only the relevant ones. Experimental re-sults show that the proposed system outperforms state-of-the-art works on the most used three publicly available benchmark datasets.},
author = {Martinel, Niki and Piciarelli, Claudio and Micheloni, Christian and Foresti, Gian Luca},
booktitle = {International Conference on Computer Vision Workshops},
doi = {10.1109/ICCVW.2015.70},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Martinel et al.{\_}International Conference on Computer Vision Workshops.pdf:pdf},
isbn = {9780769557205},
title = {{A Structured Committee for Food Recognition}},
year = {2015}
}
@article{Anthimopoulos2014,
abstract = {Computer vision-based food recognition could be used to estimate a meal's carbohydrate content for diabetic pa- tients.This study proposes amethodology for automatic foodrecog- nition, based on the bag-of-features (BoF) model. An extensive technical investigation was conducted for the identification and optimization of the best performing components involved in the BoF architecture, as well as the estimation of the corresponding parameters. For the design and evaluation of the prototype sys- tem, a visual dataset with nearly 5000 food imageswas created and organized into 11 classes. The optimized system computes dense local features, using the scale-invariant feature transform on the HSV color space, builds a visual dictionary of 10000 visual words by using the hierarchical k-means clustering and finally classifies the food images with a linear support vector machine classifier. The system achieved classification accuracy of the order of 78{\%}, thus proving the feasibility of the proposed approach in a very challenging image dataset. Index},
author = {Anthimopoulos, Marios M. and Gianola, Lauro and Scarnato, Luca and Diem, Peter and Mougiakakou, Stavroula G.},
doi = {10.1109/JBHI.2014.2308928},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Anthimopoulos et al.{\_}IEEE Journal of Biomedical and Health Informatics(2).pdf:pdf},
issn = {21682194},
journal = {IEEE Journal of Biomedical and Health Informatics},
keywords = {Bag of features (BoF),diabetes,feature extraction,food recognition,image classification},
month = {jul},
number = {4},
pages = {1261--1271},
pmid = {25014934},
publisher = {IEEE},
title = {{A food recognition system for diabetic patients based on an optimized bag-of-features model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6762879},
volume = {18},
year = {2014}
}
@inproceedings{Kagaya2014,
abstract = {In this paper, we apply a convolutional neural network (CNN) to the tasks of detecting and recognizing food images. Be-cause of the wide diversity of types of food, image recog-nition of food items is generally very difficult. However, deep learning has been shown recently to be a very powerful image recognition technique, and CNN is a state-of-the-art approach to deep learning. We applied CNN to the tasks of food detection and recognition through parameter optimiza-tion. We constructed a dataset of the most frequent food items in a publicly available food-logging system, and used it to evaluate recognition performance. CNN showed signif-icantly higher accuracy than did traditional support-vector-machine-based methods with handcrafted features. In addi-tion, we found that the convolution kernels show that color dominates the feature extraction process. For food image detection, CNN also showed significantly higher accuracy than a conventional method did.},
author = {Kagaya, Hokuto and Aizawa, Kiyoharu and Ogawa, Makoto},
booktitle = {ACM Multimedia},
doi = {10.1145/2647868.2654970},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf//Kagaya, Aizawa, Ogawa{\_}ACM Multimedia.pdf:pdf},
isbn = {9781450330633},
keywords = {convolu-,deep learning,food detection,food recognition,tional neural network},
number = {2},
pages = {1085--1088},
title = {{Food Detection and Recognition Using Convolutional Neural Network}},
url = {http://dl.acm.org/citation.cfm?doid=2647868.2654970},
year = {2014}
}
@article{TaichiJoutou2009,
abstract = {Since health care on foods is drawing people's attention recently, a system that can record everyday meals easily is being awaited. In this paper, we propose an automatic food image recognition system for recording people's eating habits. In the proposed system, we use the Multiple Kernel Learning (MKL) method to integrate several kinds of image features such as color, texture and SIFT adaptively. MKL enables to estimate optimal weights to combine image features for each category. In addition, we implemented a prototype system to recognize food images taken by cellular-phone cameras. In the experiment, we have achieved the 61.34{\%} classification rate for 50 kinds of foods. To the best of our knowledge, this is the first report of a food image classification system which can be applied for practical use.},
author = {{Taichi Joutou} and {Keiji Yanai}},
doi = {10.1109/ICIP.2009.5413400},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Taichi Joutou, Keiji Yanai{\_}2009 16th IEEE International Conference on Image Processing (ICIP)(2).pdf:pdf},
isbn = {978-1-4244-5653-6},
issn = {9781424456543},
journal = {2009 16th IEEE International Conference on Image Processing (ICIP)},
keywords = {food image,generic object recognition,multiple kernel learning},
month = {nov},
pages = {285--288},
publisher = {IEEE},
title = {{A food image recognition system with Multiple Kernel Learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5413400},
year = {2009}
}
@article{Hu1962,
abstract = {In this paper a theory of two-dimensional moment invariants for planar geometric figures is presented. A fundamental theorem is established to relate such moment invariants to the well-known algebraic invariants. Complete systems of moment invariants under translation, similitude and orthogonal transformations are derived. Some moment invariants under general two-dimensional linear transformations are also included. Both theoretical formulation and practical models of visual pattern recognition based upon these moment invariants are discussed. A simple simulation program together with its performance are also presented. It is shown that recognition of geometrical patterns and alphabetical characters independently of position, size and orientation can be accomplished. It is also indicated that generalization is possible to include invariance with parallel projection.},
author = {Hu, Ming-Kuei},
doi = {10.1109/TIT.1962.1057692},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Hu{\_}IRE Transactions on Information Theory.pdf:pdf},
isbn = {0096-1000},
issn = {0096-1000},
journal = {IRE Transactions on Information Theory},
keywords = {Artificial intelligence,Bibliographies,Character recognition,Decision theory,Distribution functions,Image analysis,Information processing,Information theory,Pattern recognition,Senior members,Shape},
pages = {179--187},
title = {{Visual pattern recognition by moment invariants}},
volume = {8},
year = {1962}
}
@article{Zhu2011,
abstract = {Given a dataset of images, we seek to automatically identify and locate perceptually similar objects. We combine two ideas to achieve this: a set of segmented objects can be partitioned into perceptually similar object classes based on global and local features; and perceptually similar object classes can be used to assess the accuracy of image segmentation. These ideas are implemented by generating multiple segmentations of each image and then learning the object class by combining different segmentations to generate optimal segmentation. We demonstrate that the proposed method can be used as part of a new dietary assessment tool to automatically identify and locate the foods in a variety of food images captured during different user studies.},
author = {Zhu, Fengqing and Bosch, Marc and Khanna, Nitin and Boushey, Carol J. and Delp, Edward J.},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Zhu et al.{\_}2011 7th International Symposium on Image and Signal Processing and Analysis (ISPA)(2).pdf:pdf},
isbn = {978-1-4577-0841-1},
issn = {1845-5921},
journal = {2011 7th International Symposium on Image and Signal Processing and Analysis (ISPA)},
number = {Ispa},
pages = {337--342},
pmid = {22127051},
title = {{Multilevel segmentation for food classification in dietary assessment}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6046629},
year = {2011}
}
@inproceedings{Lazebnik2006,
abstract = { This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting "spatial pyramid" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba{\&}{\#}146;s "gist" and Lowe{\&}{\#}146;s SIFT descriptors.},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Lazebnik, Svetlana and Schmid, Cordelia and Ponce, Jean},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2006.68},
eprint = {9411012},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Lazebnik, Schmid, Ponce{\_}Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition(2).pdf:pdf},
isbn = {0769525970},
issn = {10636919},
keywords = {Histograms,Image databases,Image recognition,Image representation,Image segmentation,Layout,Object recognition,Robustness,Shape,Spatial databases},
pages = {2169--2178},
primaryClass = {chao-dyn},
publisher = {IEEE},
title = {{Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1641019},
volume = {2},
year = {2006}
}
@article{Noronha2011,
abstract = {We introduce PlateMate, a system that allows users to take photos of their meals and receive estimates of food intake and composition. Accurate awareness of this information can help people monitor their progress towards dieting goals, but current methods for food logging via self-reporting, expert observation, or algorithmic analysis are time-consuming, expensive, or inaccurate. PlateMate crowdsources nutritional analysis from photographs using Amazon Mechanical Turk, automatically coordinating untrained workers to estimate a meal's calories, fat, carbohydrates, and protein. We present the Management framework for crowdsourcing complex tasks, which supports PlateMate's nutrition analysis workflow. Results of our evaluations show that PlateMate is nearly as accurate as a trained dietitian and easier to use for most users than traditional self-reporting.},
author = {Noronha, Jon and Hysen, Eric and Zhang, Haoqi and Gajos, Krzysztof Z},
doi = {10.1145/2047196.2047198},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Noronha et al.{\_}Proceedings of the 24th annual ACM symposium on User interface software and technology - UIST '11.pdf:pdf},
isbn = {9781450307161},
journal = {Proceedings of the 24th annual ACM symposium on User interface software and technology - UIST '11},
keywords = {crowdsourcing,human computation,ical turk,mechan-,nutrition,remote food photography},
pages = {1},
title = {{Platemate: crowdsourcing nutritional analysis from food photographs}},
url = {http://dl.acm.org/citation.cfm?doid=2047196.2047198},
year = {2011}
}
@article{VanderWalt2014,
abstract = {scikit-image is an image processing library that implements algorithms and utilities for use in research, education and industry applications. It is released under the liberal Modified BSD open source license, provides a well-documented API in the Python programming language, and is developed by an active, international team of collaborators. In this paper we highlight the advantages of open source to achieve the goals of the scikit-image library, and we showcase several real-world image processing applications that use scikit-image. More information can be found on the project homepage, http://scikit-image.org.},
archivePrefix = {arXiv},
arxivId = {1407.6245},
author = {van der Walt, St{\'{e}}fan and Sch{\"{o}}nberger, Johannes L and Nunez-Iglesias, Juan and Boulogne, Fran{\c{c}}ois and Warner, Joshua D and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony},
doi = {10.7717/peerj.453},
eprint = {1407.6245},
isbn = {2167-9843},
issn = {2167-8359},
journal = {PeerJ},
keywords = {Education,Image processing,Open source,Python,Reproducible research,Scientific programming,Visualization},
pages = {e453},
pmid = {25024921},
title = {{Scikit-image: image processing in Python}},
url = {https://peerj.com/articles/453},
volume = {2},
year = {2014}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Simonyan, Zisserman{\_}ImageNet Challenge.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
journal = {ImageNet Challenge},
pages = {1--10},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@inproceedings{Bosch2011,
abstract = {Many chronic diseases, such as heart diseases, diabetes, and obesity, can be related to diet. Hence, the need to accurately measure diet becomes imperative. We are developing methods to use image analysis tools for the identification and quantification of food consumed at a meal. In this paper we describe a new approach to food identification using several features based on local and global measures and a “voting” based late decision fusion classifier to identify the food items. Experimental results on a wide variety of food items are presented. Index},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Bosch, Marc and Zhu, Fengqing and Khanna, Nitin and Boushey, Carol J. and Delp, Edward J.},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2011.6115809},
eprint = {NIHMS150003},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Bosch et al.{\_}Proceedings - International Conference on Image Processing, ICIP(2).pdf:pdf},
isbn = {9781457713033},
issn = {15224880},
keywords = {Feature extraction,image analysis,image texture,object recognition,supervised learning},
month = {sep},
pages = {1789--1792},
pmid = {1000000221},
publisher = {IEEE},
title = {{Combining global and local features for food identification in dietary assessment}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6115809},
year = {2011}
}
@article{ParisaPouladzadehAbdulsalamYassine2015,
author = {{Abdulsalam Yassine}, Parisa Pouladzadeh and {Shervin Shirmohammadi}},
doi = {10.1007/978-3-319-23222-5},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Abdulsalam Yassine, Shervin Shirmohammadi{\_}New Trends in Image Analysis and Processing -- ICIAP 2015 Workshops.pdf:pdf},
isbn = {9783319232218},
issn = {16113349},
journal = {New Trends in Image Analysis and Processing -- ICIAP 2015 Workshops},
keywords = {calorie measurement,food detection,food image dataset},
pages = {441--448},
title = {{FooDD: Food Detection Dataset for Calorie Measurement Using Food Images}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-23222-5{\_}54},
volume = {9281},
year = {2015}
}
@inproceedings{Matsuda2012a,
abstract = {In this paper, we propose a two-step method to recognize multiple-food images by detecting candidate regions with several methods and classifying them with various kinds of features. In the first step, we detect several candidate re- gions by fusing outputs of several region detectors including Felzenszwalb's deformable part model (DPM) [1], a circle de- tector and the JSEG region segmentation. In the second step, we apply a feature-fusion-based food recognition method for bounding boxes of the candidate regions with various kinds of visual features including bag-of-features of SIFT and CSIFT with spatial pyramid (SP-BoF), histogram of oriented gradi- ent (HoG), and Gabor texture features. In the experiments, we estimated ten food candidates for multiple-food images in the descending order of the confi- dence scores. As results, we have achieved the 55.8{\%} classi- fication rate, which improved the baseline result in case of us- ing only DPM by 14.3 points, for a multiple-food image data set. This demonstrates that the proposed two-step method is effective for recognition of multiple-food images.},
author = {Matsuda, Yuji and Hoashi, Hajime and Yanai, Keiji},
booktitle = {Proceedings - IEEE International Conference on Multimedia and Expo},
doi = {10.1109/ICME.2012.157},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Matsuda, Hoashi, Yanai{\_}Proceedings - IEEE International Conference on Multimedia and Expo(2).pdf:pdf},
isbn = {978-1-4673-1659-0},
issn = {19457871},
keywords = {multiple kernel learning,multiple-food image,region detection,window search},
month = {jul},
pages = {25--30},
publisher = {IEEE},
title = {{Recognition of multiple-food images by detecting candidate regions}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6298369},
year = {2012}
}
@inproceedings{Wang2015,
abstract = {This paper deals with automatic systems for image recipe recognition. For this purpose, we compare and evaluate leading vision-based and text-based technologies on a new very large multimodal dataset (UPMC Food-101) containing about 100,000 recipes for a total of 101 food categories. Each item in this dataset is represented by one image plus textual information. We present deep experiments of recipe recognition on our dataset using visual, textual information and fusion. Additionally, we present experiments with text-based embedding technology to represent any food word in a semantical continuous space. We also compare our dataset features with a twin dataset provided by ETHZ university: we revisit their data collection protocols and carry out transfer learning schemes to highlight similarities and differences between both datasets. Finally, we propose a real application for daily users to identify recipes. This application is a web search engine that allows any mobile device to send a query image and retrieve the most relevant recipes in our dataset.},
author = {Wang, Xin and Kumar, Devinder and Thome, Nicolas and Cord, Matthieu and Precioso, Frederic},
booktitle = {2015 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2015},
doi = {10.1109/ICMEW.2015.7169757},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Wang et al.{\_}2015 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2015(2).pdf:pdf},
isbn = {9781479970797},
keywords = {Accuracy,Feature extraction,Google,HTML,Internet,Protocols,Training,UPMC Food-101,Visualization,Web search engine,computer vision,data collection protocols,food categories,food technology,image fusion,image recipe recognition,image recognition,image retrieval,mobile device,multimodal dataset,multimodal food dataset,query image,relevant recipe retrieval,search engines,semantical continuous space,text analysis,text-based embedding technology,text-based technology,transfer learning schemes,vision-based technology},
month = {jun},
pages = {1--6},
publisher = {IEEE},
title = {{Recipe recognition with large multimodal food dataset}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7169757},
year = {2015}
}
@inproceedings{Bay2006,
abstract = {Abstract. In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Ro- bust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (in casu, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper presents experimental results on a standard evaluation set, as well as on imagery obtained in the context of a real-life object recognition application. Both show SURF's strong performance.},
author = {Bay, Herbert and Tuytelaars, Tinne and {Van Gool}, Luc},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/11744023_32},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Bay, Tuytelaars, Van Gool{\_}Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture No.pdf:pdf},
isbn = {3540338322},
issn = {03029743},
pages = {404--417},
pmid = {16081019},
title = {{SURF: Speeded up robust features}},
volume = {3951 LNCS},
year = {2006}
}
@article{Nguyen2014,
abstract = {This paper proposes food image classification methods exploiting both local appearance and global structural information of food objects. The contribution of the paper is threefold. First, non-redundant local binary pattern (NRLBP) is used to describe the local appearance information of food objects. Second, the structural information of food objects is represented by the spatial relationship between interest points and encoded using a shape context descriptor formed from those interest points. Third, we propose two methods of integrating appearance and structural information for the description and classification of food images. We evaluated the proposed methods on two datasets. Experimental results verified that the combination of local appearance and structural features can improve classification performance. {\textcopyright} 2014 Elsevier B.V.},
author = {Nguyen, Duc Thanh and Zong, Zhimin and Ogunbona, Philip O. and Probst, Yasmine and Li, Wanqing},
doi = {10.1016/j.neucom.2014.03.017},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Nguyen et al.{\_}Neurocomputing.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Food image classification,Local binary pattern,Non-redundant local binary pattern,Shape context},
pages = {242--251},
title = {{Food image classification using local appearance and global structural information}},
url = {http://www.sciencedirect.com/science/article/pii/S0925231214004317},
volume = {140},
year = {2014}
}
@article{Pouladzadeh2014,
abstract = {As people across the globe are becoming more interested in watching their weight, eating more healthy, and avoiding obesity, a system that can measure calories and nutrition in every day meals can be very useful. In this paper, we propose a food calorie and nutrition measurement system that can help patients and dietitians to measure and manage daily food intake. Our system is built on food image processing and uses nutritional fact tables. Recently, there has been an increase in the usage of personal mobile technology such as smartphones or tablets, which users carry with them practically all the time. Via a special calibration technique, our system uses the built-in camera of such mobile devices and records a photo of the food before and after eating it to measure the consumption of calorie and nutrient components. Our results show that the accuracy of our system is acceptable and it will greatly improve and facilitate current manual calorie measurement techniques.},
author = {Pouladzadeh, Parisa and Shirmohammadi, Shervin and Al-Maghrabi, Rana},
doi = {10.1109/TIM.2014.2303533},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Pouladzadeh, Shirmohammadi, Al-Maghrabi{\_}IEEE Transactions on Instrumentation and Measurement(2).pdf:pdf},
issn = {00189456},
journal = {IEEE Transactions on Instrumentation and Measurement},
keywords = {Calorie measurement,food image processing,obesity management},
month = {aug},
number = {8},
pages = {1947--1956},
publisher = {IEEE},
title = {{Measuring calorie and nutrition from food image}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6748066},
volume = {63},
year = {2014}
}
@article{Kawano2013,
abstract = {We propose a mobile food recognition system the poses of which are$\backslash$nestimating calorie and nutritious of foods and recording a user's$\backslash$neating habits. Since all the processes on image recognition performed$\backslash$non a smart-phone, the system does not need to send images to a server$\backslash$nand runs on an ordinary smartphone in a real-time way. To recognize$\backslash$nfood items, a user draws bounding boxes by touching the screen first,$\backslash$nand then the system starts food item recognition within the indicated$\backslash$nbounding boxes. To recognize them more accurately, we segment each$\backslash$nfood item region by GrubCut, extract a color histogram and SURF-based$\backslash$nbag-of-features, and finally classify it into one of the fifty food$\backslash$ncategories with linear SVM and fast 2 kernel. In addition, the system$\backslash$nestimates the direction of food regions where the higher SVM output$\backslash$nscore is expected to be obtained, show it as an arrow on the screen$\backslash$nin order to ask a user to move a smartphone camera. This recognition$\backslash$nprocess is performed repeatedly about once a second. We implemented$\backslash$nthis system as an Android smartphone application so as to use multiple$\backslash$nCPU cores effectively for real-time recognition. In the experiments,$\backslash$nwe have achieved the 81.55{\%} classification rate for the top 5 category$\backslash$ncandidates when the ground-truth bounding boxes are given. In addition,$\backslash$nwe obtained positive evaluation by user study compared to the food$\backslash$nrecording system without object recognition.},
author = {Kawano, Y and Yanai, K},
doi = {10.1109/CVPRW.2013.5},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Kawano, Yanai{\_}Computer Vision and Pattern Recognition Workshops (CVPRW), 2013 IEEE Conference on.pdf:pdf},
isbn = {9780769549903},
issn = {2160-7508},
journal = {Computer Vision and Pattern Recognition Workshops (CVPRW), 2013 IEEE Conference on},
keywords = {behavioural sciences computing;feature extraction;},
pages = {1--7},
title = {{Real-Time Mobile Food Recognition System}},
year = {2013}
}
@article{Vedaldi2010,
author = {Vedaldi, A and Zisserman, A},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Vedaldi, Zisserman{\_}{\{}IEEE{\}} Int. Conf. on Computer Vision and Pattern Recognition.pdf:pdf},
isbn = {9781424469857},
journal = {{\{}IEEE{\}} Int. Conf. on Computer Vision and Pattern Recognition},
number = {Xx},
pages = {3539--3546},
title = {{Efficient Additive Kernels via Explicit Feature Maps}},
volume = {XX},
year = {2010}
}
@article{Zhang2010,
abstract = {Aims: To estimate the global health expenditure on diabetes among people aged 20-79 years for the years 2010 and 2030. Methods: Country-by-country expenditures for 193 countries, expressed in United States Dollars (USD) and in International Dollars (ID), were estimated based on the country's age-sex specific diabetes prevalence and population estimates, per capita health expenditures, and health expenditure ratios per person with and without diabetes. Diabetes prevalence was estimated from studies in 91 countries. Population estimates and health expenditures were from the United Nations and the World Health Organization. The health expenditure ratios were estimated based on utilization and cost data of a large health plan in the U.S. Diabetes expenditures for the year 2030 were projected by considering future changes in demographics and urbanization. Results: The global health expenditure on diabetes is expected to total at least USD 376 billion or ID 418 billion in 2010 and USD 490 billion or ID 561 billion in 2030. Globally, 12{\%} of the health expenditures and USD 1330 (ID 1478) per person are anticipated to be spent on diabetes in 2010. The expenditure varies by region, age group, gender, and country's income level. Conclusions: Diabetes imposes an increasing economic burden on national health care systems worldwide. More prevention efforts are needed to reduce this burden. Meanwhile, the very low expenditures per capita in poor countries indicate that more resources are required to provide basic diabetes care in such settings.},
author = {Zhang, Ping and Zhang, Xinzhi and Brown, Jonathan and Vistisen, Dorte and Sicree, Richard and Shaw, Jonathan and Nichols, Gregory},
doi = {10.1016/j.diabres.2010.01.026},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Zhang et al.{\_}Diabetes Research and Clinical Practice.pdf:pdf},
isbn = {1872-8227 (Electronic)$\backslash$n0168-8227 (Linking)},
issn = {01688227},
journal = {Diabetes Research and Clinical Practice},
keywords = {Diabetes,Economic burden,Health expenditure},
number = {3},
pages = {293--301},
pmid = {20171754},
publisher = {Elsevier Ireland Ltd},
title = {{Global healthcare expenditure on diabetes for 2010 and 2030}},
url = {http://dx.doi.org/10.1016/j.diabres.2010.01.026},
volume = {87},
year = {2010}
}
@article{Lowe2004,
abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
archivePrefix = {arXiv},
arxivId = {cs/0112017},
author = {Lowe, David G},
doi = {http://dx.doi.org/10.1023/B:VISI.0000029664.99615.94},
eprint = {0112017},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Lowe{\_}International Journal of Computer Vision.pdf:pdf},
isbn = {1568811012},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
number = {2},
pages = {91--110},
pmid = {20064111},
primaryClass = {cs},
title = {{Distinctive image features from scale invariant keypoints}},
url = {http://portal.acm.org/citation.cfm?id=996342},
volume = {60},
year = {2004}
}
@inproceedings{Farinella2014,
abstract = {The classification of food images is an interesting and challenging problem since the high variability of the image content which makes the task difficult for current state-of-the-art classification methods. The image representation to be employed in the classification engine plays an important role. We believe that texture features have been not properly considered in this application domain. This paper points out, through a set of experiments, that textures are fundamental to properly recognize different food items. For this purpose the bag of visual words model (BoW) is employed. Images are processed with a bank of rotation and scale invariant filters and then a small codebook of Textons is built for each food class. The learned class-based Textons are hence collected in a single visual dictionary. The food images are represented as visual words distributions (Bag of Textons) and a Support Vector Machine is used for the classification stage. The experiments demonstrate that the image representation based on Bag of Textons is more accurate than existing (and more complex) approaches in classifying the 61 classes of the Pittsburgh Fast-Food Image Dataset.},
author = {Farinella, Giovanni Maria and Moltisanti, Marco and Battiato, Sebastiano},
booktitle = {2014 IEEE International Conference on Image Processing, ICIP 2014},
doi = {10.1109/ICIP.2014.7026055},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Farinella, Moltisanti, Battiato{\_}2014 IEEE International Conference on Image Processing, ICIP 2014(2).pdf:pdf},
isbn = {9781479957514},
keywords = {Bag of Words,Food Classification,Textons},
month = {oct},
pages = {5212--5216},
publisher = {IEEE},
title = {{Classifying food images represented as Bag of Textons}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7026055},
year = {2014}
}
@article{Menachemi2011,
abstract = {The Health Information Technology for Economic and Clinical Health (HITECH) Act of 2009 that was signed into law as part of the "stimulus package" represents the largest US initiative to date that is designed to encourage widespread use of electronic health records (EHRs). In light of the changes anticipated from this policy initiative, the purpose of this paper is to review and summarize the literature on the benefits and drawbacks of EHR systems. Much of the literature has focused on key EHR functionalities, including clinical decision support systems, computerized order entry systems, and health information exchange. Our paper describes the potential benefits of EHRs that include clinical outcomes (eg, improved quality, reduced medical errors), organizational outcomes (eg, financial and operational benefits), and societal outcomes (eg, improved ability to conduct research, improved population health, reduced costs). Despite these benefits, studies in the literature highlight drawbacks associated with EHRs, which include the high upfront acquisition costs, ongoing maintenance costs, and disruptions to workflows that contribute to temporary losses in productivity that are the result of learning a new system. Moreover, EHRs are associated with potential perceived privacy concerns among patients, which are further addressed legislatively in the HITECH Act. Overall, experts and policymakers believe that significant benefits to patients and society can be realized when EHRs are widely adopted and used in a "meaningful" way.},
archivePrefix = {arXiv},
arxivId = {0710.4428v1},
author = {Menachemi, Nir and Collum, Taleah H.},
doi = {10.2147/RMHP.S12985},
eprint = {0710.4428v1},
isbn = {1179-1594},
issn = {11791594},
journal = {Risk Management and Healthcare Policy},
keywords = {Computerized Order Entry,EHR,Health Information Exchange,Health Information Technology,Hitech},
pages = {47--55},
pmid = {22312227},
title = {{Benefits and drawbacks of electronic health record systems}},
volume = {4},
year = {2011}
}
@article{Oliphant2007,
abstract = {By itself, Python is an excellent "steering" language for scientific codes written in other languages. However, with additional basic tools, Python transforms into a high-level language suited for scientific and engineering code that's often fast enough to be immediately useful but also flexible enough to be sped up with additional extensions.},
author = {Oliphant, Travis E},
issn = {1521-9615},
journal = {Computing in Science and Engineering},
pages = {10--20},
title = {{SciPy: Open source scientific tools for Python}},
url = {http://www.scipy.org/},
volume = {9},
year = {2007}
}
@article{Burke2011a,
abstract = {Self-monitoring is the centerpiece of behavioral weight loss intervention programs. This article presents a systematic review of the literature on three components of self-monitoring in behavioral weight loss studies: diet, exercise, and self-weighing. This review included articles that were published between 1993 and 2009 that reported on the relationship between weight loss and these self-monitoring strategies. Of the 22 studies identified, 15 focused on dietary self-monitoring, one on self-monitoring exercise, and six on self-weighing. A wide array of methods was used to perform self-monitoring; the paper diary was used most often. Adherence to self-monitoring was reported most frequently as the number of diaries completed or the frequency of log-ins or reported weights. The use of technology, which included the Internet, personal digital assistants, and electronic digital scales were reported in five studies. Descriptive designs were used in the earlier studies whereas more recent reports involved prospective studies and randomized trials that examined the effect of self-monitoring on weight loss. A significant association between self-monitoring and weight loss was consistently found; however, the level of evidence was weak because of methodologic limitations. The most significant limitations of the reviewed studies were the homogenous samples and reliance on self-report. In all but two studies, the samples were predominantly white and women. This review highlights the need for studies in more diverse populations, for objective measures of adherence to self-monitoring, and for studies that establish the required dose of self-monitoring for successful outcomes. ?? 2011 American Dietetic Association.},
author = {Burke, Lora E. and Wang, Jing and Sevick, Mary Ann},
doi = {10.1016/j.jada.2010.10.008},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Burke, Wang, Sevick{\_}Journal of the American Dietetic Association.pdf:pdf},
isbn = {1878-3570 (Electronic)$\backslash$n0002-8223 (Linking)},
issn = {00028223},
journal = {Journal of the American Dietetic Association},
number = {1},
pages = {92--102},
pmid = {21185970},
publisher = {Elsevier Inc.},
title = {{Self-Monitoring in Weight Loss: A Systematic Review of the Literature}},
url = {http://dx.doi.org/10.1016/j.jada.2010.10.008},
volume = {111},
year = {2011}
}
@article{Lichtman1992,
abstract = {BACKGROUND AND METHODS: Some obese subjects repeatedly fail to lose weight even though they report restricting their caloric intake to less than 1200 kcal per day. We studied two explanations for this apparent resistance to diet--low total energy expenditure and underreporting of caloric intake--in 224 consecutive obese subjects presenting for treatment. Group 1 consisted of nine women and one man with a history of diet resistance in whom we evaluated total energy expenditure and its main thermogenic components and actual energy intake for 14 days by indirect calorimetry and analysis of body composition. Group 2, subgroups of which served as controls in the various evaluations, consisted of 67 women and 13 men with no history of diet resistance. RESULTS: Total energy expenditure and resting metabolic rate in the subjects with diet resistance (group 1) were within 5 percent of the predicted values for body composition, and there was no significant difference between groups 1 and 2 in the thermic effects of food and exercise. Low energy expenditure was thus excluded as a mechanism of self-reported diet resistance. In contrast, the subjects in group 1 underreported their actual food intake by an average (+/- SD) of 47 +/- 16 percent and overreported their physical activity by 51 +/- 75 percent. Although the subjects in group 1 had no distinct psychopathologic characteristics, they perceived a genetic cause for their obesity, used thyroid medication at a high frequency, and described their eating behavior as relatively normal (all P {\textless} 0.05 as compared with group 2). CONCLUSIONS: The failure of some obese subjects to lose weight while eating a diet they report as low in calories is due to an energy intake substantially higher than reported and an overestimation of physical activity, not to an abnormality in thermogenesis.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Lichtman, S W and Pisarska, K and Berman, E R and Pestone, M and Dowling, H and Offenbacher, E and Weisel, H and Heshka, S and Matthews, D E and Heymsfield, S B},
doi = {10.1056/NEJM199212313272701},
eprint = {arXiv:1011.1669v3},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Lichtman et al.{\_}The New England Journal of Medicine.pdf:pdf},
isbn = {0028-4793 (Print)},
issn = {0028-4793},
journal = {The New England Journal of Medicine},
number = {27},
pages = {1893--1898},
pmid = {1454084},
title = {{Discrepancy between self-reported and actual caloric intake and exercise in obese subjects.}},
volume = {327},
year = {1992}
}
@article{Pedregosa2012,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1201.0490},
isbn = {1532-4435},
issn = {15324435},
journal = {{\ldots} of Machine Learning {\ldots}},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://scikit-learn.org/stable/},
volume = {12},
year = {2012}
}
@article{Chen2009,
abstract = {We introduce the first visual dataset of fast foods with a total of 4,545 still images, 606 stereo pairs, 303 360° videos for structure from motion, and 27 privacy-preserving videos of eating events of volunteers. This work was motivated by research on fast food recognition for dietary assessment. The data was collected by obtaining three instances of 101 foods from 11 popular fast food chains, and capturing images and videos in both restaurant conditions and a controlled lab setting. We benchmark the dataset using two standard approaches, color histogram and bag of SIFT features in conjunction with a discriminative classifier. Our dataset and the benchmarks are designed to stimulate research in this area and will be released freely to the research community.},
author = {Chen, Mei and Dhingra, Kapil and Wu, Wen and Yang, Lei and Sukthankar, Rahul and Yang, Jie},
doi = {10.1109/ICIP.2009.5413511},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Chen et al.{\_}Proceedings - International Conference on Image Processing, ICIP.pdf:pdf},
isbn = {9781424456543},
issn = {15224880},
journal = {Proceedings - International Conference on Image Processing, ICIP},
keywords = {Food image dataset,Object recognition},
pages = {289--292},
title = {{PFID: Pittsburgh Fast-food Image Dataset}},
year = {2009}
}
@article{Arandjelovic2012,
abstract = {The objective of this work is object retrieval in large scale image datasets, where the object is specified by an image query and retrieval should be immediate at run time in the manner of Video Google [28]. We make the following three contributions: (i) a new method to compare SIFT descriptors (RootSIFT) which yields superior performance without increasing process- ing or storage requirements; (ii) a novel method for query expansion where a richer model for the query is learnt discriminatively in a form suited to immediate retrieval through efficient use of the inverted index; (iii) an improve- ment of the image augmentationmethod proposed by Turcot and Lowe [29], where only the augmenting features which are spatially consistent with the augmented image are kept. We evaluate these three methods over a number of stan- dard benchmark datasets (Oxford Buildings 5k and 105k, and Paris 6k) and demonstrate substantial improvements in retrieval performance whilst maintaining immediate re- trieval speeds. Combining these complementary meth- ods achieves a new state-of-the-art performance on these datasets.},
author = {Arandjelovic, Relja and Zisserman, Andrew},
doi = {10.1109/CVPR.2012.6248018},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Arandjelovic, Zisserman{\_}IEEE Conference on computer vision and Pattern Recognition.pdf:pdf},
isbn = {9781467312288},
issn = {9781467312288},
journal = {IEEE Conference on computer vision and Pattern Recognition},
number = {April},
pages = {2911--2918},
title = {{Three things everyone should know to improve object retrieval c}},
year = {2012}
}
@article{Bradski2000,
abstract = {OpenCV is an open-source, computer-vision library for extracting and processing meaningful data from images.},
author = {Bradski, G},
doi = {10.1111/0023-8333.50.s1.10},
isbn = {1044-789X},
issn = {1044-789X},
journal = {Dr Dobbs Journal of Software Tools},
pages = {120--125},
pmid = {22118455},
title = {{The OpenCV Library}},
url = {http://opencv.org/},
volume = {25},
year = {2000}
}
@inproceedings{Yanai2015,
author = {Yanai, K and Kawano, Y},
booktitle = {Multimedia Expo Workshops (ICMEW), 2015 IEEE International Conference on},
doi = {10.1109/ICMEW.2015.7169816},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Yanai, Kawano{\_}Multimedia Expo Workshops (ICMEW), 2015 IEEE International Conference on.pdf:pdf},
isbn = {978-1-4799-7079-7},
keywords = {feature extraction;food products;image classificat},
month = {jun},
pages = {1--6},
publisher = {IEEE},
title = {{Food image recognition using deep convolutional network with pre-training and fine-tuning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7169816},
year = {2015}
}
@article{Hunter2007,
abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.},
author = {Hunter, John D.},
doi = {10.1109/MCSE.2007.55},
isbn = {1521-9615 VO - 9},
issn = {15219615},
journal = {Computing in Science and Engineering},
number = {3},
pages = {99--104},
pmid = {1000044628},
title = {{Matplotlib: A 2D graphics environment}},
volume = {9},
year = {2007}
}
@article{Szegedy2015,
author = {Szegedy, C and Liu, W and Jia, Y and Sermanet, P},
doi = {10.1109/CVPR.2015.7298594},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Szegedy et al.{\_}Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.pdf:pdf},
isbn = {9781467369640},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {1--9},
title = {{Going deeper with convolutions}},
url = {/citations?view{\_}op=view{\_}citation{\&}continue=/scholar?hl=ja{\&}as{\_}sdt=0,5{\&}scilib=1{\&}citilm=1{\&}citation{\_}for{\_}view=KtmM-dAAAAAJ:JV2RwH3{\_}ST0C{\&}hl=ja{\&}oi=p},
year = {2015}
}
@article{Bolanos2016,
abstract = {The development of automatic nutrition diaries, which would allow to keep track objectively of everything we eat, could enable a whole new world of possibilities for people concerned about their nutrition patterns. With this purpose, in this paper we propose the first method for simultaneous food localization and recognition. Our method is based on two main steps, which consist in, first, produce a food activation map on the input image (i.e. heat map of probabilities) for generating bounding boxes proposals and, second, recognize each of the food types or food-related objects present in each bounding box. We demonstrate that our proposal, compared to the most similar problem nowadays - object localization, is able to obtain high precision and reasonable recall levels with only a few bounding boxes. Furthermore, we show that it is applicable to both conventional and egocentric images.},
archivePrefix = {arXiv},
arxivId = {1604.07953},
author = {Bola{\~{n}}os, Marc and Radeva, Petia},
eprint = {1604.07953},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Bola{\~{n}}os, Radeva{\_}Unknown.pdf:pdf},
pages = {2--7},
title = {{Simultaneous Food Localization and Recognition}},
url = {http://arxiv.org/abs/1604.07953},
year = {2016}
}
@article{Jia2014a,
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU ({\$}\backslashapprox{\$} 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
archivePrefix = {arXiv},
arxivId = {1408.5093},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
doi = {10.1145/2647868.2654889},
eprint = {1408.5093},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Jia et al.{\_}Proceedings of the ACM International Conference on Multimedia.pdf:pdf},
isbn = {9781450330633},
issn = {10636919},
journal = {Proceedings of the ACM International Conference on Multimedia},
keywords = {computation,computer vision,corresponding authors,machine learning,neural networks,open source,parallel},
pages = {675--678},
title = {{Caffe: Convolutional Architecture for Fast Feature Embedding}},
url = {http://arxiv.org/abs/1408.5093},
year = {2014}
}
@inproceedings{Yang2010,
abstract = {Food recognition is difficult because food items are de-formable objects that exhibit significant variations in appearance. We believe the key to recognizing food is to exploit the spatial relationships between different ingredients (such as meat and bread in a sandwich). We propose a new representation for food items that calculates pairwise statistics between local features computed over a soft pixel-level segmentation of the image into eight ingredient types. We accumulate these statistics in a multi-dimensional histogram, which is then used as a feature vector for a discriminative classifier. Our experiments show that the proposed representation is significantly more accurate at identifying food than existing methods.},
author = {Yang, Shulin and Chen, Mei and Pomerleau, Dean and Sukthankar, Rahul},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5539907},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Yang et al.{\_}Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition(2).pdf:pdf},
isbn = {9781424469840},
issn = {10636919},
keywords = {Computer vision,Histograms,Image edge detection,Image segmentation,Object recognition,Pixel,Predictive models,Robots,Shape,Statistics,discriminative classifier,feature vector,food recognition,image segmentation,local features,multi-dimensional histogram,object recognition,pairwise statistics,soft pixel-level segmentation},
month = {jun},
pages = {2249--2256},
publisher = {IEEE},
title = {{Food recognition using statistics of pairwise local features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5539907},
year = {2010}
}
@article{Chen2012,
abstract = {Computer-aided food identification and quantity estimation have caught more attention than before due to the growing concern of health and obesity. The identification problem is usually defined as an image categorization or classification problem and several researches on this topic have been proposed. In this paper, we address the issues of feature descriptors in the food identification problem and introduce a preliminary approach for the quantity es- timation using depth information. Sparse coding is utilized in the SIFT and Local binary pattern feature descriptors, and these fea- tures combined with Gabor and color features are used to represent food items. A multi-label SVM classifier is trained for each fea- ture, and these classifiers are combined with multi-class Adaboost algorithm. For evaluation, 50 major categories of worldwide food are used, and each category contains 100 photographs from differ- ent sources, such as photos taken manually or from Internet web albums. An overall accuracy of 68.3{\%} is achieved, and success at top-N candidates achieved 80.6{\%}, 84.8{\%}, and 90.9{\%} accuracy accordingly when N equals 2, 3, and 5, thus making mobile appli- cation practical. The experimental results show that the proposed methods greatly improve the performance of original SIFT and LBP feature descriptors. On the other hand, for quantity estimation us- ing depth information, a straight forward method is proposed for certain food, while transparent food ingredients such as pure water and},
author = {Chen, Mei-Yun and Yang, Yung-Hsiang and Ho, Chia-Ju and Wang, Shih-Han and Liu, Shane-Ming and Chang, Eugene and Yeh, Che-Hua and Ouhyoung, Ming},
doi = {10.1145/2407746.2407775},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Chen et al.{\_}SIGGRAPH Asia.pdf:pdf},
isbn = {9781450319157},
journal = {SIGGRAPH Asia},
pages = {1--4},
title = {{Automatic Chinese food identification and quantity estimation}},
url = {http://dl.acm.org/citation.cfm?doid=2407746.2407775},
year = {2012}
}
@inproceedings{Beijbom2015,
abstract = {Logging food and calorie intake has been shown to facilitate weight management. Unfortunately, current food logging methods are time-consuming and cumbersome, which limits their effectiveness. To address this limitation, we present an automated computer vision system for logging food and calorie intake using images. We focus on the "restaurant" scenario, which is often a challenging aspect of diet management. We introduce a key insight that addresses this problem specifically: restaurant plates are often both nutritionally and visually consistent across many servings. This insight provides a path to robust calorie estimation from a single RGB photograph: using a database of known food items together with restaurant-specific classifiers, calorie estimation can be achieved through identification followed by calorie lookup. As demonstrated on a challenging Menu-Match dataset and an existing third party dataset, our approach outperforms previous computer vision methods and a commercial calorie estimation app. Our Menu-Match dataset of realistic restaurant meals is made publicly available.},
author = {Beijbom, Oscar and Joshi, Neel and Morris, Dan and Saponas, Scott and Khullar, Siddharth},
booktitle = {Proceedings - 2015 IEEE Winter Conference on Applications of Computer Vision, WACV 2015},
doi = {10.1109/WACV.2015.117},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Beijbom et al.{\_}Proceedings - 2015 IEEE Winter Conference on Applications of Computer Vision, WACV 2015(2).pdf:pdf},
isbn = {9781479966820},
keywords = {Computer vision,Databases,Estimation,Feature extraction,Image color analysis,Menu-Match dataset,Standards,Visualization,automated computer vision system,calorie estimation,calorie intake logging,catering industry,computer vision,diet management,image classification,image colour analysis,restaurant meals,restaurant plates,restaurant-specific classifiers,restaurant-specific food logging,single RGB photograph},
month = {jan},
pages = {844--851},
publisher = {IEEE},
title = {{Menu-match: Restaurant-specific food logging from images}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7045971},
year = {2015}
}
@inproceedings{Hoashi2010a,
abstract = {Recognition of food images is challenging due to their diversity and practical for health care on foods for people. In this paper, we propose an automatic food image recognition system for 85 food categories by fusing various kinds of image features including bag-of-features{\~{}}(BoF), color histogram, Gabor features and gradient histogram with Multiple Kernel Learning{\~{}}(MKL). In addition, we implemented a prototype system to recognize food images taken by cellular-phone cameras. In the experiment, we have achieved the 62.52{\&}{\#}x025; classification rate for 85 food categories.},
author = {Hoashi, Hajime and Joutou, Taichi and Yanai, Keiji},
booktitle = {Proceedings - 2010 IEEE International Symposium on Multimedia, ISM 2010},
doi = {10.1109/ISM.2010.51},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Hoashi, Joutou, Yanai{\_}Proceedings - 2010 IEEE International Symposium on Multimedia, ISM 2010(2).pdf:pdf},
isbn = {9780769542171},
keywords = {Feature fusion,Food image recognition,Multiple kernel learning},
month = {dec},
pages = {296--301},
publisher = {IEEE},
title = {{Image recognition of 85 food categories by feature fusion}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5693856},
year = {2010}
}
@article{Dehais2015,
abstract = {Diet-related chronic diseases severely affect personal and global health. However, managing or treating these diseases currently requires long training and high personal involvement to succeed. Computer vision systems could assist with the assessment of diet by detecting and recognizing different foods and their portions in images. We propose novel methods for detecting a dish in an image and segmenting its contents with and without user interaction. All methods were evaluated on a database of over 1600 manually annotated images. The dish detection scored an average of 99{\%} accuracy with a .2s/image run time, while the automatic and semi-automatic dish segmentation methods reached average accuracies of 88{\%} and 91{\%} respectively, with an average run time of .5s/image, outperforming competing solutions.},
author = {Dehais, Joachim and Anthimopoulos, Marios and Mougiakakou, Stavroula},
doi = {10.1007/978-3-319-23222-5},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Dehais, Anthimopoulos, Mougiakakou{\_}New Trends in Image Analysis and Processing -- ICIAP 2015 Workshops.pdf:pdf},
isbn = {9783319232218},
issn = {16113349},
journal = {New Trends in Image Analysis and Processing -- ICIAP 2015 Workshops},
keywords = {computer,diabetes,diet assessment,image segmentation,obesity},
pages = {433--440},
title = {{Dish Detection and Segmentation for Dietary Assessment on Smartphones}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-23222-5{\_}53},
volume = {9281},
year = {2015}
}
@article{Ristin2014,
abstract = {In recent years, large image data sets such as "ImageNet", "TinyImages" or ever-growing social networks like "Flickr" have emerged, posing new challenges to image classification that were not apparent in smaller image sets. In particular, the efficient handling of dynamically growing data sets, where not only the amount of training images, but also the number of classes increases over time, is a relatively unexplored problem. To remedy this, we introduce Nearest Class Mean Forests (NCMF), a variant of Random Forests where the decision nodes are based on nearest class mean (NCM) classification. NCMFs not only outperform conventional random forests, but are also well suited for integrating new classes. To this end, we propose and compare several approaches to incorporate data from new classes, so as to seamlessly extend the previously trained forest instead of re-training them from scratch. In our experiments, we show that NCMFs trained on small data sets with 10 classes can be extended to large data sets with 1000 classes without significant loss of accuracy compared to training from scratch on the full data.},
author = {Ristin, Marko and Guillaumin, Matthieu and Gall, Juergen and {Van Gool}, Luc},
doi = {10.1109/CVPR.2014.467},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Ristin et al.{\_}Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Incremental learning,image classification,large-scale,nearest class mean classifier,random forest},
month = {mar},
number = {99},
pages = {3654--3661},
publisher = {IEEE},
title = {{Incremental learning of NCM forests for large-scale image classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7164339},
volume = {PP},
year = {2014}
}
@article{Thendral2014a,
abstract = {In this paper, we presented two segmentation methods. Edge based and color based detection methods were used to segment images of orange fruits obtained under natural lighting conditions. Twenty digitized images of orange fruits were randomly selected from the Internet in order to find an orange in each image and to determine its location. We compared the results of both segmentation results and the color based segmentation outperforms the edge based segmentation in all aspects. The MATLAB image processing toolbox is used for the computation and comparison results are shown in the segmented image results.},
author = {Thendral, R. and Suhasini, A. and Senthil, N.},
doi = {10.1109/ICCSP.2014.6949884},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Thendral, Suhasini, Senthil{\_}International Conference on Communication and Signal Processing, ICCSP 2014 - Proceedings.pdf:pdf},
isbn = {9781479933587},
journal = {International Conference on Communication and Signal Processing, ICCSP 2014 - Proceedings},
keywords = {Orange harvesting,color based segmentation,edge based segmentation,machine vision},
pages = {463--466},
title = {{A comparative analysis of edge and color based segmentation for orange fruit recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6949884},
year = {2014}
}
@article{McKinney2010,
abstract = {In this paper we are concerned with the practical issues of working with data sets common to finance, statistics, and other related fields. pandas is a new library which aims to facilitate working with these data sets and to provide a set of fundamental building blocks for implementing statistical models. We will discuss specific design issues encountered in the course of developing pandas with relevant examples and some comparisons with the R language. We conclude by discussing possible future directions for statistical computing and data analysis using Python.},
author = {McKinney, Wes},
isbn = {0440877763224},
journal = {Proceedings of the 9th Python in Science Conference},
pages = {51--56},
pmid = {1000224767},
title = {{Data Structures for Statistical Computing in Python}},
url = {http://conference.scipy.org/proceedings/scipy2010/mckinney.html},
year = {2010}
}
@article{Zhu2015,
abstract = {We propose a method for dietary assessment to automatically identify and locate food in a variety of images captured during controlled and natural eating events. Two concepts are combined to achieve this: a set of segmented objects can be partitioned into perceptually similar object classes based on global and local features; and perceptually similar object classes can be used to assess the accuracy of image segmentation. These ideas are implemented by generating multiple segmentations of an image to select stable segmentations based on the classifier's confidence score assigned to each segmented image region. Automatic segmented regions are classified using a multichannel feature classification system. For each segmented region, multiple feature spaces are formed. Feature vectors in each of the feature spaces are individually classified. The final decision is obtained by combining class decisions from individual feature spaces using decision rules. We show improved accuracy of segmenting food images with classifier feedback.},
author = {Zhu, Fengqing and Bosch, Marc and Khanna, Nitin and Boushey, Carol J. and Delp, Edward J.},
doi = {10.1109/JBHI.2014.2304925},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Zhu et al.{\_}IEEE Journal of Biomedical and Health Informatics(2).pdf:pdf},
issn = {21682194},
journal = {IEEE Journal of Biomedical and Health Informatics},
keywords = {Dietary assessment,image analysis,image features,image segmentation,object recognition},
month = {jan},
number = {1},
pages = {377--388},
pmid = {25561457},
publisher = {IEEE},
title = {{Multiple hypotheses image segmentation and classification with application to dietary assessment}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6733271},
volume = {19},
year = {2015}
}
@article{Kawano2014a,
abstract = {We propose a mobile food recognition system, FoodCam, the purposes of which are estimating calorie and nutrition of foods and recording a user's eating habits. In this paper, we propose image recognition methods which are suitable for mobile devices. The proposed method enables real-time food image recognition on a consumer smartphone. This characteristic is completely different from the existing systems which require to send images to an image recognition server. To recognize food items, a user draws bounding boxes by touching the screen first, and then the system starts food item recognition within the indicated bounding boxes. To recognize them more accurately, we segment each food item region by GrubCut, extract image features and finally classify it into one of the one hundred food categories with a linear SVM. As image features, we adopt two kinds of features: one is the combination of the standard bag-of-features and color histograms with $\chi$2 kernel feature maps, and the other is a HOG patch descriptor and a color patch descriptor with the state-of-the-art Fisher Vector representation. In addition, the system estimates the direction of food regions where the higher SVM output score is expected to be obtained, and it shows the estimated direction in an arrow on the screen in order to ask a user to move a smartphone camera. This recognition process is performed repeatedly and continuously. We implemented this system as a standalone mobile application for Android smartphones so as to use multiple CPU cores effectively for real-time recognition. In the experiments, we have achieved the 79.2 {\%} classification rate for the top 5 category candidates for a 100-category food dataset with the ground-truth bounding boxes when we used HOG and color patches with the Fisher Vector coding as image features. In addition, we obtained positive evaluation by a user study compared to the food recording system without object recognition. {\textcopyright} 2014 Springer Science+Business Media New York.},
author = {Kawano, Yoshiyuki and Yanai, Keiji},
doi = {10.1007/s11042-014-2000-8},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Kawano, Yanai{\_}Multimedia Tools and Applications.pdf:pdf},
isbn = {13807501},
issn = {13807501},
journal = {Multimedia Tools and Applications},
keywords = {Dietary recording,Fisher vector,Food recognition,Mobile image recognition,Smartphone application},
pages = {5263--5287},
title = {{FoodCam: A real-time food recognition system on a smartphone}},
year = {2014}
}
@article{Aizawa2013,
author = {Aizawa, Kiyoharu and Maruyama, Yuto and Li, He and Morikawa, Chamin and {De Silva}, G. C.},
doi = {10.1109/TMM.2013.2271474},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Aizawa et al.{\_}IEEE Transactions on Multimedia(2).pdf:pdf},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Bayesian estimation,Food log,Food record,Image processing,Lifelog,Multimedia},
month = {dec},
number = {8},
pages = {2176--2185},
publisher = {IEEE},
title = {{Food balance estimation by using personal dietary tendencies in a multimedia food log}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6548059},
volume = {15},
year = {2013}
}
@article{Jia2014,
abstract = {OBJECTIVE: Accurate estimation of food portion size is of paramount importance in dietary studies. We have developed a small, chest-worn electronic device called eButton which automatically takes pictures of consumed foods for objective dietary assessment. From the acquired pictures, the food portion size can be calculated semi-automatically with the help of computer software. The aim of the present study is to evaluate the accuracy of the calculated food portion size (volumes) from eButton pictures.$\backslash$n$\backslash$nDESIGN: Participants wore an eButton during their lunch. The volume of food in each eButton picture was calculated using software. For comparison, three raters estimated the food volume by viewing the same picture. The actual volume was determined by physical measurement using seed displacement.$\backslash$n$\backslash$nSETTING: Dining room and offices in a research laboratory.$\backslash$n$\backslash$nSUBJECTS: Seven lab member volunteers.$\backslash$n$\backslash$nRESULTS: Images of 100 food samples (fifty Western and fifty Asian foods) were collected and each food volume was estimated from these images using software. The mean relative error between the estimated volume and the actual volume over all the samples was -2{\textperiodcentered}8 {\%} (95 {\%} CI -6{\textperiodcentered}8 {\%}, 1{\textperiodcentered}2 {\%}) with sd of 20{\textperiodcentered}4 {\%}. For eighty-five samples, the food volumes determined by computer differed by no more than 30 {\%} from the results of actual physical measurements. When the volume estimates by the computer and raters were compared, the computer estimates showed much less bias and variability.$\backslash$n$\backslash$nCONCLUSIONS: From the same eButton pictures, the computer-based method provides more objective and accurate estimates of food volume than the visual estimation method.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Jia, Wenyan and Chen, Hsin-Chen and Yue, Yaofeng and Li, Zhaoxin and Fernstrom, John and Bai, Yicheng and Li, Chengliu and Sun, Mingui},
doi = {10.1017/S1368980013003236},
eprint = {NIHMS150003},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Jia et al.{\_}Public health nutrition.pdf:pdf},
isbn = {2122633255},
issn = {1475-2727},
journal = {Public health nutrition},
keywords = {Adult,Diet Surveys,Energy Intake,Female,Food,Humans,Lunch,Male,Photography,Portion Size,Regression Analysis,Reproducibility of Results,Size Perception,Thorax},
number = {8},
pages = {1671--81},
pmid = {24476848},
title = {{Accuracy of food portion size estimation from digital pictures acquired by a chest-worn camera.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4152011{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {17},
year = {2014}
}
@inproceedings{Zong2010,
abstract = {This paper proposes a food image classification method using local textural patterns and their global structure to describe the food image. In this paper, a visual codebook of local textural patterns is created by employing Scale Invariant Feature Transformation (SIFT) interest point detector with the Local Binary Pattern (LBP) feature. In addition to describing the food image using local texture, the global structure of the food object is represented as the spatial distribution of the local textural structures and encoded using shape context. We evaluated the proposed method on the Pittsburgh Fast-Food Image (PFI) dataset. Experimental results showed that the proposed method could obtain better performance than the baseline experiment on the PFI dataset.},
author = {Zong, Zhimin and Nguyen, Duc Thanh and Ogunbona, Philip and Li, Wanqing},
booktitle = {Proceedings - 2010 IEEE International Symposium on Multimedia, ISM 2010},
doi = {10.1109/ISM.2010.37},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Zong et al.{\_}Proceedings - 2010 IEEE International Symposium on Multimedia, ISM 2010(2).pdf:pdf},
isbn = {9780769542171},
keywords = {Food classification,Local binary pattern,Shape context},
month = {dec},
pages = {204--211},
publisher = {IEEE},
title = {{On the combination of local texture and global structure for food classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5693842},
year = {2010}
}
@inproceedings{Kusumoto2013,
author = {Kusumoto, Riko and Han, Xian Hua and Chen, Yen Wei},
booktitle = {Proceedings of the 2013 6th International Conference on Biomedical Engineering and Informatics, BMEI 2013},
doi = {10.1109/BMEI.2013.6747060},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Kusumoto, Han, Chen{\_}Proceedings of the 2013 6th International Conference on Biomedical Engineering and Informatics, BMEI 2013(2).pdf:pdf},
isbn = {978-1-4799-2761-6},
keywords = {food,formatting,image recognation,sparse coding},
month = {dec},
pages = {851--855},
publisher = {IEEE},
title = {{Sparse model in hierarchic spatial structure for food image recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6747060},
year = {2013}
}
@inproceedings{Bettadapura2015,
abstract = {The pervasiveness of mobile cameras has resulted in a dramatic increase in food photos, which are pictures re- flecting what people eat. In this paper, we study how tak- ing pictures of what we eat in restaurants can be used for the purpose of automating food journaling. We propose to leverage the context of where the picture was taken, with ad- ditional information about the restaurant, available online, coupled with state-of-the-art computer vision techniques to recognize the food being consumed. To this end, we demon- strate image-based recognition of foods eaten in restaurants by training a classifier with images from restaurant's on- line menu databases. We evaluate the performance of our system in unconstrained, real-world settings with food im- ages taken in 10 restaurants across 5 different types of food (American, Indian, Italian, Mexican and Thai). 1.},
archivePrefix = {arXiv},
arxivId = {1510.02078},
author = {Bettadapura, Vinay and Thomaz, Edison and Parnami, Aman and Abowd, Gregory D. and Essa, Irfan},
booktitle = {Proceedings - 2015 IEEE Winter Conference on Applications of Computer Vision, WACV 2015},
doi = {10.1109/WACV.2015.83},
eprint = {1510.02078},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Bettadapura et al.{\_}Proceedings - 2015 IEEE Winter Conference on Applications of Computer Vision, WACV 2015.pdf:pdf},
isbn = {9781479966820},
pages = {580--587},
title = {{Leveraging context to support automated food recognition in restaurants}},
url = {http://www.vbettadapura.com/egocentric/food/},
year = {2015}
}
@article{Rocha2008,
abstract = {We propose a system to solve a multi-class produce categorization problem. For that, we use statistical color, texture, and structural appearance descriptors (bag-of-features). As the best combination setup is not known for our problem, we combine several individual features from the state-of-the-art in many different ways to assess how they interact to improve the overall accuracy of the system. We validate the system using an image data set collected on our local fruits and vegetables distribution center.},
author = {Rocha, Anderson and Hauagge, Daniel C. and Wainer, Jacques and Goldenstein, Siome},
doi = {10.1109/SIBGRAPI.2008.9},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Rocha et al.{\_}Proceedings - 21st Brazilian Symposium on Computer Graphics and Image Processing, SIBGRAPI 2008.pdf:pdf},
isbn = {9780769533582},
issn = {1530-1834},
journal = {Proceedings - 21st Brazilian Symposium on Computer Graphics and Image Processing, SIBGRAPI 2008},
pages = {3--10},
title = {{Automatic produce classification from images using color, texture and appearance cues}},
url = {http://www.ic.unicamp.br/{~}siome/papers/rocha-sib08.pdf},
year = {2008}
}
@article{zhang2015SOD,
author = {Zhang, Jianming and Sclaroff, Stan and Lin, Zhe and Shen, Xiaohui and Price, Brian},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Zhang et al.{\_}IEEE Conference on Computer Vision and Pattern Recognition(CVPR).pdf:pdf},
journal = {IEEE Conference on Computer Vision and Pattern Recognition(CVPR)},
title = {{Unconstrained Salient Object Detection via Proposal Subset Optimization}},
url = {http://cs-people.bu.edu/jmzhang/sod.html},
year = {2016}
}
@article{Arivazhagan2010,
abstract = {The computer vision strategies used to recognize a fruit rely on four basic features which characterize the object: intensity, color, shape and texture. This paper proposes an efficient fusion of color and texture features for fruit recognition. The recognition is done by the minimum distance classifier based upon the statistical and co-occurrence features derived from the Wavelet transformed sub- bands. Experimental results on a database of about 2635 fruits from 15 different classes confirm the effectiveness of the proposed approach.},
author = {Arivazhagan, S and Shebiah, R Newlin and Nidhyanandhan, S Selva and Ganesan, L},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Arivazhagan et al.{\_}Information Sciences.pdf:pdf},
journal = {Information Sciences},
keywords = {co occurrence features,fruit recognition,texture,wavelet transform},
number = {2},
pages = {90--94},
title = {{Fruit Recognition using Color and Texture Features}},
url = {http://cisjournal.org/archive/vol1no1/vol1no1{\_}12.pdf},
volume = {1},
year = {2010}
}
@article{Almaghrabi2012a,
abstract = {In this paper, a food nutrition and energy intake recognition system for medical purposes is proposed. This system is built based on food image processing and shape recognition in addition to nutritional fact tables. Recently, countless studies suggested that the usage of technology such as smartphones may enhance the treatments for obesity and overweight patients. Via a special technique, the system records a photo of the food before and after eating in order to estimate the consumption calorie of the selected food and its nutrients components. Our system presents a new instrument in food intake measuring systems which can be useful and effective in obesity management.},
author = {Almaghrabi, Rana and Villalobos, Gregorio and Pouladzadeh, Parisa and Shirmohammadi, Shervin},
doi = {10.1109/I2MTC.2012.6229581},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Almaghrabi et al.{\_}2012 Ieee I2Mtc.pdf:pdf},
isbn = {9781457717710},
issn = {1091-5281},
journal = {2012 Ieee I2Mtc},
keywords = {Calories measurement,Image processing,Shape recognition,obesity management},
pages = {366--370},
title = {{A novel method for measuring nutrition intake based on food image}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6229581},
year = {2012}
}
@article{Pishva2000,
author = {Pishva, D. and Hirakawa, K. and Kawai, A. and Shiino, T.},
doi = {10.1109/ICOSP.2000.891642},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Pishva et al.{\_}WCC 2000 - ICSP 2000. 2000 5th International Conference on Signal Processing Proceedings. 16th World Computer Congress (2).pdf:pdf},
isbn = {0-7803-5747-7},
journal = {WCC 2000 - ICSP 2000. 2000 5th International Conference on Signal Processing Proceedings. 16th World Computer Congress 2000},
keywords = {bread,cluster analysis,color,color algorithms assume a,could have any,distribution,fixed facial orientation,machine vision,orientation,our application bread samples,since in,texture,we developed an indigenous},
pages = {840--844},
publisher = {IEEE},
title = {{A unified image segmentation approach with application to bread recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=891642},
volume = {2},
year = {2000}
}
@inproceedings{Shimoda2015,
abstract = {We propose a CNN-based food image segmentation which requires no pixel-wise annotation. The proposed method consists of food region proposals by selective search and bounding box clustering, back propagation based saliency map estimation with the CNN model fine-tuned with the UEC-FOOD100 dataset, GrabCut guided by the estimated saliency maps and region integration by non-maximum suppression. In the experiments, the proposed method outperformed RCNN regarding food region detection as well as the PASCAL VOC detection task.},
author = {Shimoda, Wataru and Yanai, Keiji},
booktitle = {New Trends in Image Analysis and Processing -- ICIAP 2015 Workshops},
doi = {10.1007/978-3-319-23222-5_55},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf//Shimoda, Yanai{\_}New Trends in Image Analysis and Processing -- ICIAP 2015 Workshops.pdf:pdf},
isbn = {9783319232218},
issn = {16113349},
keywords = {Convolutional neural network,Deep learning,Food segmentation,UEC-FOOD},
pages = {449--457},
title = {{CNN-based food image segmentation without pixel-wise annotation}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-23222-5{\_}55},
volume = {9281},
year = {2015}
}
@article{Wazumi2013,
author = {Wazumi, Minami and Han, Xian-Hua and Chen, Yen-Wei},
doi = {10.1109/SII.2013.6776730},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Wazumi, Han, Chen{\_}Proceedings of the 2013 IEEESICE International Symposium on System Integration(2).pdf:pdf},
isbn = {978-1-4799-2625-1},
journal = {Proceedings of the 2013 IEEE/SICE International Symposium on System Integration},
keywords = {Educational institutions,Encoding,Feature extraction,Image recognition,Image representation,Sc-based codebook model,Vectors,Visualization,codebook-based mode-bag-of feature,codebook-based model,dietary life,dish recognition,food image recognition,food-log system,health and safety,health care,healthy life,image recognition,image representation,menu content auto-recognition,mobile phone,sparse-coding,spatial pyramid matching,unhealthy diets},
month = {dec},
pages = {482--485},
publisher = {IEEE},
title = {{Food recognition using Codebook-based model with sparse-coding}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6776730},
year = {2013}
}
@article{Farinella2015,
abstract = {It is well-known that people love food. However, an insane diet can cause problems in the general health of the people. Since health is strictly linked to the diet, advanced computer vision tools to recognize food images (e.g. acquired with mobile/wearable cameras), as well as their properties (e.g., calories), can help the diet monitoring by providing useful information to the experts (e.g., nutritionists) to assess the food intake of patients (e.g., to combat obesity). The food recognition is a challenging task since the food is intrinsically deformable and presents high variability in appearance. Image representation plays a fundamental role. To properly study the peculiarities of the image representation in the food application context, a benchmark dataset is needed. These facts motivate the work presented in this paper. In this work we introduce the UNICT-FD889 dataset. It is the first food image dataset composed by over 800 distinct plates of food which can be used as benchmark to design and compare representation models of food images. We exploit the UNICT-FD889 dataset for Near Duplicate Image Retrieval (NDIR) purposes by comparing three standard state-of-the-art image descriptors: Bag of Textons, PRICoLBP and SIFT. Results confirm that both textures and colors are fundamental properties in food representation. Moreover the experiments point out that the Bag of Textons representation obtained considering the color domain is more accurate than the other two approaches for NDIR.},
archivePrefix = {arXiv},
arxivId = {1410.2488},
author = {Farinella, Giovanni Maria and Allegra, Dario and Stanco, Filippo},
doi = {10.1007/978-3-319-16199-0_41},
eprint = {1410.2488},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Farinella, Allegra, Stanco{\_}Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture N.pdf:pdf},
isbn = {9783319161983},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Food dataset,Food recognition,Near duplicate image retrieval,PRICoLBP,SIFT,Textons},
pages = {584--599},
pmid = {25317426},
title = {{A benchmark dataset to study the representation of food images}},
volume = {8927},
year = {2015}
}
@article{Hillestad2005,
abstract = {To broadly examine the potential health and financial benefits of health information technology (HIT), this paper compares health care with the use of IT in other industries. It estimates potential savings and costs of widespread adoption of electronic medical record (EMR) systems, models important health and safety benefits, and concludes that effective EMR implementation and networking could eventually save more than {\$}81 billion annually--by improving health care efficiency and safety--and that HIT-enabled prevention and management of chronic disease could eventually double those savings while increasing health and other social benefits. However, this is unlikely to be realized without related changes to the health care system.},
author = {Hillestad, Richard and Bigelow, James and Bower, Anthony and Girosi, Federico and Meili, Robin and Scoville, Richard and Taylor, Roger},
doi = {10.1377/hlthaff.24.5.1103},
isbn = {1544-5208},
issn = {0278-2715},
journal = {Health affairs (Project Hope)},
keywords = {Aged,Cost Control,Cost Control: economics,Delivery of Health Care,Delivery of Health Care: economics,Delivery of Health Care: organization {\&} administra,Diffusion of Innovation,Efficiency, Organizational,Health Expenditures,Health Expenditures: trends,Humans,Medical Records Systems, Computerized,Middle Aged,Quality of Health Care,United States},
number = {5},
pages = {1103--17},
pmid = {16162551},
title = {{Can electronic medical record systems transform health care? Potential health benefits, savings, and costs.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16162551},
volume = {24},
year = {2005}
}
@article{Kitamura2008,
abstract = {In this paper, a food-logging system that can distinguish food images from other images, analyze the food balance, and visualize the log is presented. The image processing is based on feature vectors consisting of color histograms, DCT coefficients, detected image patterns and so forth. Support Vector Machine (SVM) was used to detect food images and to analyze the food balance. Experimental results show that the food image extraction presents above 88{\%} of accuray and the food balance estimation is achieved with more than 73{\%} of accuracy.},
author = {Kitamura, Keigo and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
doi = {10.1145/1459359.1459548},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Kitamura, Yamasaki, Aizawa{\_}ACM international conference on Multimedia.pdf:pdf},
isbn = {9781605583037},
journal = {ACM international conference on Multimedia},
pages = {999},
title = {{Food log by analyzing food images}},
url = {http://portal.acm.org/citation.cfm?doid=1459359.1459548},
year = {2008}
}
@article{Kawano2015,
abstract = {In this paper, we propose a novel effective framework to ex-pand an existing image dataset automatically leveraging existing cat-egories and crowdsourcing. Especially, in this paper, we focus on ex-pansion on food image data set. The number of food categories is un-countable, since foods are different from a place to a place. If we have a Japanese food dataset, it does not help build a French food recognition system directly. That is why food data sets for different food cultures have been built independently category so far. Then, in this paper, we propose to leverage existing knowledge on food of other cultures by a generic " foodness " classifier and domain adaptation. This can enable us not only to built other-cultured food datasets based on an original food image dataset automatically, but also to save as much crowd-sourcing costs as possible. In the experiments, we show the effectiveness of the proposed method over the baselines.},
author = {Kawano, Yoshiyuki and Yanai, Keiji},
doi = {10.1007/978-3-319-16199-0_1},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Kawano, Yanai{\_}Lecture Notes in Computer Science.pdf:pdf},
isbn = {9783319161983},
issn = {16113349},
journal = {Lecture Notes in Computer Science},
keywords = {Adaptive SVM,Crowd-sourcing,Dataset expansion,Domain adaptation,Food image,Foodness},
pages = {3--17},
title = {{Automatic expansion of a food image dataset leveraging existing categories with domain adaptation}},
volume = {8927},
year = {2015}
}
@inproceedings{Bossard2014,
abstract = {In this paper we address the problem of automatically rec- ognizing pictured dishes. To this end, we introduce a novel method to mine discriminative parts using Random Forests (rf), which allows us to mine for parts simultaneously for all classes and to share knowledge among them. To improve efficiency of mining and classification, we only consider patches that are aligned with image superpixels, which we call components. To measure the performance of our rf component mining for food recognition, we introduce a novel and challenging dataset of 101 food categories, with 101'000 images. With an average accuracy of 50.76{\%}, our model outperforms alternative classification methods except for cnn, including svm classification on Improved Fisher Vectors and existing discriminative part-mining algorithms by 11.88{\%} and 8.13{\%}, re- spectively. On the challenging mit-Indoor dataset, our method compares nicely to other s-o-a component-based classification methods.},
archivePrefix = {arXiv},
arxivId = {10.1007/978-3-319-10599-4{\_}29},
author = {Bossard, Lukas and Guillaumin, Matthieu and {Van Gool}, Luc},
booktitle = {Lecture Notes in Computer Science},
doi = {10.1007/978-3-319-10599-4_29},
eprint = {978-3-319-10599-4{\_}29},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Bossard, Guillaumin, Van Gool{\_}Lecture Notes in Computer Science.pdf:pdf},
isbn = {9783319105987},
issn = {16113349},
keywords = {Discriminative part mining,Food recognition,Image classification,Random Forest},
number = {PART 6},
pages = {446--461},
primaryClass = {10.1007},
title = {{Food-101 - Mining discriminative components with random forests}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-10599-4{\_}29},
volume = {8694 LNCS},
year = {2014}
}
@inproceedings{DeSilva2011,
abstract = {Food images have been receiving increased attention in recent dietary control methods. We present the current status of our web-based system that can be used as a dietary management support system by ordinary Internet users. The system analyzes image archives of the user to identify images of meals. Further image analysis determines the nutritional composition of these meals and stores the data to form a Foodlog. The user can view the data in different formats, and also edit the data to correct any mistakes that occurred during image analysis. This paper presents detailed analysis of the performance of the current system and proposes an improvement of analysis by pre-classification and personalization. As a result, the accuracy of food balance estimation is significantly improved.},
author = {{De Silva}, Gamhewage C. and Aizawa, Kiyoharu},
booktitle = {Proceedings - IEEE International Conference on Multimedia and Expo},
doi = {10.1109/ICME.2011.6012167},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/De Silva, Aizawa{\_}Proceedings - IEEE International Conference on Multimedia and Expo(2).pdf:pdf},
isbn = {9781612843490},
issn = {19457871},
keywords = {FoodLog,Meal summary,clustering,meal image analysis,segmentation},
month = {jul},
pages = {1--6},
publisher = {IEEE},
title = {{Clustering meal images in a web-based dietary management system}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6012167},
year = {2011}
}
@article{Kawano2014,
abstract = {Abstract In this paper, we report the feature obtained from the Deep Convolutional Neural Network boosts food recognition accuracy greatly by integrating it with conventional hand-crafted image features, Fisher Vectors with HoG and Color patches. In the experiments, we have achieved 72.26{\%} as the top-1 accuracy and 92.00{\%} as the top-5 accuracy for the 100-class food dataset, UEC-FOOD100, which outperforms the best classification accuracy of this dataset reported so far, 59.6{\%}, greatly.},
author = {Kawano, Yoshiyuki and Yanai, Keiji},
doi = {10.1145/2638728.2641339},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Kawano, Yanai{\_}ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp).pdf:pdf},
isbn = {9781450330473},
journal = {ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp)},
keywords = {Author Keywords food recognition,Deep Convolutional Neural Network,Fisher Vector},
pages = {589--593},
title = {{Food Image Recognition with Deep Convolutional Features}},
url = {http://dx.doi.org/10.1145/2638728.2641339},
year = {2014}
}
@article{Pouladzadeh2015a,
abstract = {The integration of multimedia-assisted healthcare systems with could-computing services and mobile technologies has led to increased accessibility for healthcare providers and patients. Utilizing cloud computing infrastructures and virtualization technologies allows for the transformation of traditional healthcare systems that demand manual care and monitoring to more salient, automatic and cost effective systems. The goal of this paper is to develop a multimedia-assisted mobile healthcare application using cloud-computing virtualization technologies. We consider calorie measurement as an example healthcare application that can benefit from cloud-computing virtualization technology. The key functionalities of our application entail image segmentation, image processing and deep learning algorithms for food classification and recognition. Client side devices (e.g. smartphones, tablets etc.) have limitations in handling time sensitive and computationally intensive algorithms pertained to our application. Image processing and deep learning algorithms, used in food recognition and calorie measurement, consume devices' batteries quickly, which is inconvenient for the user. It is also very challenging for client side devices to scale for large number of data and images, as needed for food recognition. The entire process is time-consuming and inefficient and discomforting from users' perspective and may deter them from using the application. In this paper, we address these challenges by proposing a virtualization mechanism in cloud computing that utilizes the Android architecture. Android allows for parting an application into activities run by the front-end user and services run by the back-end tasks. In the proposed virtualization mechanism, we use both the hosted and the hypervisor models to publish our Android-based food recognition and calorie measurement application in the cloud. By so doing, the users of our application can control their virtual smartphone operations through a dedicated client application installed on their smartphones, while the processing of the application continue to run on the virtual Android image even if the user is disconnected due to any unexpected event. We have performed several experiments to validate our mechanism. Specifically, we have run our deep learning and image processing algorithms for food recognition on different configuration platforms on both the cloud and local server connected to the mobile. The results show that the accuracy of the system with the virtualization mechanism is more than 94.33 {\%} compared to 87.16 {\%} when we run the application locally. Also, with our virtualization mechanism the results are processed 49 {\%} faster than the case of running the application locally.},
author = {Pouladzadeh, Parisa and Peddi, Sri Vijay Bharat and Kuhad, Pallavi and Yassine, Abdulsalam and Shirmohammadi, Shervin},
doi = {10.1007/s10586-015-0468-2},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf//Pouladzadeh et al.{\_}Cluster Computing.pdf:pdf},
isbn = {1058601504},
issn = {15737543},
journal = {Cluster Computing},
keywords = {Cloud computing,Food recognition,Healthcare,Multimedia,Virtualization},
month = {sep},
number = {3},
pages = {1099--1110},
publisher = {Springer US},
title = {{A virtualization mechanism for real-time multimedia-assisted mobile food recognition application in cloud computing}},
url = {http://link.springer.com/10.1007/s10586-015-0468-2},
volume = {18},
year = {2015}
}
@article{Pishva2000a,
abstract = {We describe a unique shape based segmentation and color distribution analysis approach that can be used in a machine vision based cash register system for commodity pricing of hand made breads. Systematic sorting and classification of bread samples according to their shapes, sizes, textures and surface color distribution is explored. In this paper, we extracted true object color from bread sample images and developed our analytical approach based on how humans usually distinguish one object from another. We also considered the similarities and differences that exist between this application and other image analysis applications such as hand written Chinese character recognition and human face detection methods. Because of the diversity of our samples, some identification procedures were just shape and size analyses while other samples required textural analysis, color and surface color distribution analyses.},
author = {Pishva, D. and Hirakawa, K. and Kawai, A. and Shiino, T.},
doi = {10.1109/ICOSP.2000.891642},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Pishva et al.{\_}WCC 2000 - ICSP 2000. 2000 5th International Conference on Signal Processing Proceedings. 16th World Computer Congress (3).pdf:pdf},
isbn = {0-7803-5747-7},
journal = {WCC 2000 - ICSP 2000. 2000 5th International Conference on Signal Processing Proceedings. 16th World Computer Congress 2000},
keywords = {bread,cluster analysis,color,color algorithms assume a,could have any,distribution,fixed facial orientation,machine vision,orientation,our application bread samples,since in,texture,we developed an indigenous},
pages = {840--844},
title = {{Shape Based Segmentation and Color Distribution Analysis with Application to Bread Recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=891642},
volume = {2},
year = {2000}
}
@misc{Pouladzadeh2014a,
abstract = {As people across the globe are becoming more interested in watching their weight, eating more healthily, and avoiding obesity, a system that can measure calories and nutrition in everyday meals can be very useful. Recently, due to ubiquity of mobile devices such as smart phones, the health monitoring applications are accessible by the patients practically all the time. We have created a semi-automatic food calorie and nutrition measurement system via mobile that can help patients and dietitians to measure and manage daily food intake. While segmentation and recognition are the two main steps of a food calorie measurement system, in this paper we have focused on the recognition part and mainly the training phase of the classification algorithm. This paper presents a cloud-based Support Vector Machine (SVM) method for classifying objects in cluster. We propose a method for food recognition application that is referred to as the Cloud SVM training mechanism in a cloud computing environment with Map Reduce technique for distributed machine learning. The results show that by using cloud computing system in classification phase and updating the database periodically, the accuracy of the recognition step has increased in single food portion, non-mixed and mixed plate of food compared to LIBSVM. {\textcopyright} 2014 Springer Science+Business Media New York.},
author = {Pouladzadeh, Parisa and Shirmohammadi, Shervin and Bakirov, Aslan and Bulut, Ahmet and Yassine, Abdulsalam},
booktitle = {Multimedia Tools and Applications},
doi = {10.1007/s11042-014-2116-x},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Pouladzadeh et al.{\_}Multimedia Tools and Applications.pdf:pdf},
isbn = {13807501},
issn = {13807501},
keywords = {Calorie measurement,Cloud computing,Food image processing},
month = {jul},
number = {14},
pages = {5243--5260},
publisher = {Springer US},
title = {{Cloud-based SVM for food categorization}},
url = {http://link.springer.com/10.1007/s11042-014-2116-x},
volume = {74},
year = {2014}
}
@article{Ojala2002,
abstract = {Presents a theoretically very simple, yet efficient,$\backslash$nmultiresolution approach to gray-scale and rotation invariant texture$\backslash$nclassification based on local binary patterns and nonparametric$\backslash$ndiscrimination of sample and prototype distributions. The method is$\backslash$nbased on recognizing that certain local binary patterns, termed$\backslash$n"uniform," are fundamental properties of local image texture and their$\backslash$noccurrence histogram is proven to be a very powerful texture feature. We$\backslash$nderive a generalized gray-scale and rotation invariant operator$\backslash$npresentation that allows for detecting the "uniform" patterns for any$\backslash$nquantization of the angular space and for any spatial resolution and$\backslash$npresents a method for combining multiple operators for multiresolution$\backslash$nanalysis. The proposed approach is very robust in terms of gray-scale$\backslash$nvariations since the operator is, by definition, invariant against any$\backslash$nmonotonic transformation of the gray scale. Another advantage is$\backslash$ncomputational simplicity as the operator can be realized with a few$\backslash$noperations in a small neighborhood and a lookup table. Experimental$\backslash$nresults demonstrate that good discrimination can be achieved with the$\backslash$noccurrence statistics of simple rotation invariant local binary patterns$\backslash$n},
author = {Ojala, Timo and Pietik{\"{a}}inen, Matti and M{\"{a}}enp{\"{a}}{\"{a}}, Topi},
doi = {10.1109/TPAMI.2002.1017623},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Ojala, Pietik{\"{a}}inen, M{\"{a}}enp{\"{a}}{\"{a}}{\_}IEEE Transactions on Pattern Analysis and Machine Intelligence.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Brodatz,Contrast,Distribution,Histogram,Nonparametric,Outex,Texture analysis},
number = {7},
pages = {971--987},
title = {{Multiresolution gray-scale and rotation invariant texture classification with local binary patterns}},
volume = {24},
year = {2002}
}
@article{Kitamura2009,
abstract = {With the increase of the number of food images on the Internet, we have been developing a food-logging system which has an automated analysis function as a Web application. It can distinguish food images from other images, analyze the food balance, and visualize the log. In this paper, we demonstrate how the performance can be improved by the personalized models. Because our Web application has an interface to review and correct the food analysis results, the generation of the personalized models can be done on-line. Experimental results using two hundred images showed that the extracted image feature vectors differ from user to user but on the other hand the feature vectors and the food balance of each user have a strong correlation. Therefore, the accuracy of the food balance estimation was improved from 37{\%} to 42{\%} on average by the personalized classifier. Copyright 2009 ACM.},
author = {Kitamura, Keigo and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
doi = {10.1145/1630995.1631001},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Kitamura, Yamasaki, Aizawa{\_}Proceedings of the ACM multimedia 2009 workshop on Multimedia for cooking and eating activities - CEA '09.pdf:pdf},
isbn = {9781605587639},
journal = {Proceedings of the ACM multimedia 2009 workshop on Multimedia for cooking and eating activities - CEA '09},
keywords = {Food,Life-log,Multimedia interfaces},
pages = {23},
title = {{FoodLog: Capture, Analysis and Retrieval of Personal Food Images via Web}},
url = {http://portal.acm.org/citation.cfm?doid=1630995.1631001},
year = {2009}
}
@article{Yao2012,
abstract = {Fine-grained categorization refers to the task of classifying objects that belong to the same basic-level class (e.g. different bird species) and share similar shape or visual appearances. Most of the state-of-the-art basic-level object classification algorithms have difficulties in this challenging problem. One reason for this can be attributed to the popular codebook-based image representation, often resulting in loss of subtle image information that are critical for fine-grained classification. Another way to address this problem is to introduce human annotations of object attributes or key points, a tedious process that is also difficult to generalize to new tasks. In this work, we propose a codebook-free and annotation-free approach for fine-grained image categorization. Instead of using vector-quantized codewords, we obtain an image representation by running a high throughput template matching process using a large number of randomly generated image templates. We then propose a novel bagging-based algorithm to build a final classifier by aggregating a set of discriminative yet largely uncorrelated classifiers. Experimental results show that our method outperforms state-of-the-art classification approaches on the Caltech-UCSD Birds dataset.},
author = {Yao, Bangpeng and Bradski, Gary and Fei-Fei, Li},
doi = {10.1109/CVPR.2012.6248088},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Yao, Bradski, Fei-Fei{\_}Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3466--3473},
title = {{A codebook-free and annotation-free approach for fine-grained image categorization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6248088},
year = {2012}
}
@article{Wazumi2011,
abstract = {Email Print Request Permissions Recently, with the increasing of unhealthy diets and the attracted attention for healthy life, how to manage the dietary life is becoming more and more important. In this paper, we aim to construct a system, which can auto-recognize the menu contents from food image taken by mobile phone. As we know that the viewpoints can be varied in any direction when taking food images, and then, rotation-robust features for image representation are very important. Therefore, in this paper, we propose to extract rotation invariant features using circle-segmentation called SPIN for food recognition, and construct a Food-Log system, which records the contents of food menu, calories and nutritional value for management of the dietary life.},
author = {Wazumi, Minami and Han, Xian-hua and Ai, Danni and Chen, Yen-wei},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Wazumi et al.{\_}Computer Sciences and Convergence Information Technology (ICCIT), 2011 6th International Conference on.pdf:pdf},
isbn = {9788988678541},
journal = {Computer Sciences and Convergence Information Technology (ICCIT), 2011 6th International Conference on},
pages = {874 -- 877},
title = {{Auto-Recognition of Food Images Using SPIN Feature for Food-Log System}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6316741},
year = {2011}
}
@article{Lazar2005,
abstract = {The epidemic of obesity-associated diabetes is a major crisis in modern societies, in which food is plentiful and exercise is optional. The biological basis of this problem has been explored from evolutionary and mechanistic perspectives. Evolutionary theories, focusing on the potential survival advantages of "thrifty" genes that are now maladaptive, are of great interest but are inherently speculative and difficult to prove. Mechanistic studies have revealed numerous fat-derived molecules and a link to inflammation that, together, are hypothesized to underlie the obesity-diabetes connection and thereby represent prospective targets for therapeutic intervention},
author = {Lazar, Mitchell A},
doi = {10.1126/science.1104342},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Lazar{\_}Science.pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$r0036-8075 (Linking)},
issn = {0036-8075},
journal = {Science},
keywords = {A,Diabetes,GENES,Gene,OBESITY,Targets,exercise,food,inflammation},
number = {5708},
pages = {373--375},
pmid = {15662001},
title = {{How Obesity Causes Diabetes: Not a Tall Tale}},
url = {http://www.sciencemag.org/cgi/content/abstract/307/5708/373},
volume = {307},
year = {2005}
}
@article{VanDerWalt2011,
abstract = {In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts.},
archivePrefix = {arXiv},
arxivId = {1102.1523},
author = {{Van Der Walt}, St{\'{e}}fan and Colbert, S. Chris and Varoquaux, Ga{\"{e}}l},
doi = {10.1109/MCSE.2011.37},
eprint = {1102.1523},
isbn = {1521-9615 VO - 13},
issn = {15219615},
journal = {Computing in Science and Engineering},
keywords = {NumPy,Python,numerical computations,programming libraries,scientific programming},
number = {2},
pages = {22--30},
pmid = {1000224770},
title = {{The NumPy array: A structure for efficient numerical computation}},
volume = {13},
year = {2011}
}
@article{Christodoulidis2015,
author = {Christodoulidis, Stergios and Anthimopoulos, Marios},
doi = {10.1007/978-3-319-23222-5},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf//Christodoulidis, Anthimopoulos{\_}New Trends in Image Analysis and Processing -- ICIAP 2015 Workshops.pdf:pdf},
isbn = {978-3-319-23221-8},
journal = {New Trends in Image Analysis and Processing -- ICIAP 2015 Workshops},
keywords = {agement,convolutional neural networks,dietary man-,food recognition,machine learning},
pages = {458--465},
publisher = {Springer International Publishing},
title = {{Food Recognition for Dietary Assessment Using Deep Convolutional Neural Networks Stergios}},
url = {http://link.springer.com/10.1007/978-3-319-23222-5{\_}56 http://link.springer.com/10.1007/978-3-319-23222-5},
volume = {9281},
year = {2015}
}
@inproceedings{Ao2015,
author = {Ao, Shuang and Ling, Charles X.},
booktitle = {2015 IEEE International Conference on Data Mining Workshop (ICDMW)},
doi = {10.1109/ICDMW.2015.203},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Ao, Ling{\_}2015 IEEE International Conference on Data Mining Workshop (ICDMW)(2).pdf:pdf},
isbn = {978-1-4673-8493-3},
keywords = {Adaptation models,Computer science,Conferences,Data mining,Feature extraction,GoogLeNet,Image recognition,Training,data mining,deep representation,feature extraction,feature extractor,feature representation,food image recognition,food processing industry,food products,image recognition,image representation,negative classifier,pattern classification},
month = {nov},
pages = {1196--1203},
publisher = {IEEE},
title = {{Adapting New Categories for Food Recognition with Deep Representation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7395804},
year = {2015}
}
@article{Wen2009,
abstract = {Accurate and passive acquisition of dietary data from patients is essential for a better understanding of the etiology of obesity and development of effective weight management programs. Self-reporting is currently the main method for such data acquisition. However, studies have shown that data obtained by self-reporting seriously underestimate food intake and thus do not accurately reflect the real habitual behavior of individuals. Computer food recognition programs have not yet been developed. In this paper, we present a study for recognizing foods from videos of eating, which are directly recorded in restaurants by a web camera. From recognition results, our method then estimates food calories of intake. We have evaluated our method on a database of 101 foods from 9 food restaurants in USA and obtained promising results.},
author = {Wen, Wu and Jie, Yang},
doi = {10.1109/ICME.2009.5202718},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Wen, Jie{\_}Proceedings - 2009 IEEE International Conference on Multimedia and Expo, ICME 2009.pdf:pdf},
isbn = {9781424442911},
issn = {1945-7871},
journal = {Proceedings - 2009 IEEE International Conference on Multimedia and Expo, ICME 2009},
keywords = {Calorie estimation,Fast food recognition},
pages = {1210--1213},
title = {{Fast food recognition from videos of eating for calorie estimation}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5202718},
year = {2009}
}
@article{Arthur2007,
author = {Arthur, David and Vassilvitskii, Sergei},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Arthur, Vassilvitskii{\_}Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms.pdf:pdf},
journal = {Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms},
keywords = {The k-means method is a widely used clustering tec,its simplicity and speed are very appealing in pra,often quite dramatically.,ran- domized seeding technique,we obtain an algorithm that is $\Theta$(log k)-competitiv},
mendeley-tags = {The k-means method is a widely used clustering tec,its simplicity and speed are very appealing in pra,often quite dramatically.,ran- domized seeding technique,we obtain an algorithm that is $\Theta$(log k)-competitiv},
pages = {1027--1035},
title = {{k-means++: The Advantages of Careful Seeding}},
url = {http://portal.acm.org/citation.cfm?id=1283494},
volume = {8},
year = {2007}
}
@article{Mokdad2003,
abstract = {CONTEXT: Obesity and diabetes are increasing in the United States.$\backslash$n$\backslash$nOBJECTIVE: To estimate the prevalence of obesity and diabetes among US adults in 2001.$\backslash$n$\backslash$nDESIGN, SETTING, AND PARTICIPANTS: Random-digit telephone survey of 195 005 adults aged 18 years or older residing in all states participating in the Behavioral Risk Factor Surveillance System in 2001.$\backslash$n$\backslash$nMAIN OUTCOME MEASURES: Body mass index, based on self-reported weight and height and self-reported diabetes.$\backslash$n$\backslash$nRESULTS: In 2001 the prevalence of obesity (BMI {\textgreater} or =30) was 20.9{\%} vs 19.8{\%} in 2000, an increase of 5.6{\%}. The prevalence of diabetes increased to 7.9{\%} vs 7.3{\%} in 2000, an increase of 8.2{\%}. The prevalence of BMI of 40 or higher in 2001 was 2.3{\%}. Overweight and obesity were significantly associated with diabetes, high blood pressure, high cholesterol, asthma, arthritis, and poor health status. Compared with adults with normal weight, adults with a BMI of 40 or higher had an odds ratio (OR) of 7.37 (95{\%} confidence interval [CI], 6.39-8.50) for diagnosed diabetes, 6.38 (95{\%} CI, 5.67-7.17) for high blood pressure, 1.88 (95{\%} CI,1.67-2.13) for high cholesterol levels, 2.72 (95{\%} CI, 2.38-3.12) for asthma, 4.41 (95{\%} CI, 3.91-4.97) for arthritis, and 4.19 (95{\%} CI, 3.68-4.76) for fair or poor health.$\backslash$n$\backslash$nCONCLUSIONS: Increases in obesity and diabetes among US adults continue in both sexes, all ages, all races, all educational levels, and all smoking levels. Obesity is strongly associated with several major health risk factors.},
author = {Mokdad, Ali H and Ford, Earl S and Bowman, Barbara A and Dietz, William H and Vinicor, Frank and Bales, Virginia S and Marks, James S},
doi = {10.1001/jama.289.1.76.},
isbn = {0098-7484 (Print)},
issn = {0098-7484},
journal = {JAMA : the journal of the American Medical Association},
number = {1},
pages = {76--9},
pmid = {12503980},
title = {{Prevalence of obesity, diabetes, and obesity-related health risk factors.}},
volume = {289},
year = {2003}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide detailed a analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Russakovsky et al.{\_}International Journal of Computer Vision.pdf:pdf},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
number = {3},
pages = {211--252},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
