Automatically generated by Mendeley Desktop 1.16.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Bettadapura2015,
abstract = {The pervasiveness of mobile cameras has resulted in a dramatic increase in food photos, which are pictures re- flecting what people eat. In this paper, we study how tak- ing pictures of what we eat in restaurants can be used for the purpose of automating food journaling. We propose to leverage the context of where the picture was taken, with ad- ditional information about the restaurant, available online, coupled with state-of-the-art computer vision techniques to recognize the food being consumed. To this end, we demon- strate image-based recognition of foods eaten in restaurants by training a classifier with images from restaurant's on- line menu databases. We evaluate the performance of our system in unconstrained, real-world settings with food im- ages taken in 10 restaurants across 5 different types of food (American, Indian, Italian, Mexican and Thai). 1.},
archivePrefix = {arXiv},
arxivId = {1510.02078},
author = {Bettadapura, Vinay and Thomaz, Edison and Parnami, Aman and Abowd, Gregory D. and Essa, Irfan},
booktitle = {Proceedings - 2015 IEEE Winter Conference on Applications of Computer Vision, WACV 2015},
doi = {10.1109/WACV.2015.83},
eprint = {1510.02078},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Bettadapura et al.{\_}Proceedings - 2015 IEEE Winter Conference on Applications of Computer Vision, WACV 2015.pdf:pdf},
isbn = {9781479966820},
pages = {580--587},
title = {{Leveraging context to support automated food recognition in restaurants}},
url = {http://www.vbettadapura.com/egocentric/food/},
year = {2015}
}
@article{Rocha2008,
abstract = {We propose a system to solve a multi-class produce categorization problem. For that, we use statistical color, texture, and structural appearance descriptors (bag-of-features). As the best combination setup is not known for our problem, we combine several individual features from the state-of-the-art in many different ways to assess how they interact to improve the overall accuracy of the system. We validate the system using an image data set collected on our local fruits and vegetables distribution center.},
author = {Rocha, Anderson and Hauagge, Daniel C. and Wainer, Jacques and Goldenstein, Siome},
doi = {10.1109/SIBGRAPI.2008.9},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Rocha et al.{\_}Proceedings - 21st Brazilian Symposium on Computer Graphics and Image Processing, SIBGRAPI 2008.pdf:pdf},
isbn = {9780769533582},
issn = {1530-1834},
journal = {Proceedings - 21st Brazilian Symposium on Computer Graphics and Image Processing, SIBGRAPI 2008},
pages = {3--10},
title = {{Automatic produce classification from images using color, texture and appearance cues}},
url = {http://www.ic.unicamp.br/{~}siome/papers/rocha-sib08.pdf},
year = {2008}
}
@inproceedings{Bosch2011,
abstract = {Many chronic diseases, such as heart diseases, diabetes, and obesity, can be related to diet. Hence, the need to accurately measure diet becomes imperative. We are developing methods to use image analysis tools for the identification and quantification of food consumed at a meal. In this paper we describe a new approach to food identification using several features based on local and global measures and a “voting” based late decision fusion classifier to identify the food items. Experimental results on a wide variety of food items are presented. Index},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Bosch, Marc and Zhu, Fengqing and Khanna, Nitin and Boushey, Carol J. and Delp, Edward J.},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2011.6115809},
eprint = {NIHMS150003},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Bosch et al.{\_}Proceedings - International Conference on Image Processing, ICIP(2).pdf:pdf},
isbn = {9781457713033},
issn = {15224880},
keywords = {Feature extraction,image analysis,image texture,object recognition,supervised learning},
month = {sep},
pages = {1789--1792},
pmid = {1000000221},
publisher = {IEEE},
title = {{Combining global and local features for food identification in dietary assessment}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6115809},
year = {2011}
}
@inproceedings{Matsuda2012a,
abstract = {In this paper, we propose a two-step method to recognize multiple-food images by detecting candidate regions with several methods and classifying them with various kinds of features. In the first step, we detect several candidate re- gions by fusing outputs of several region detectors including Felzenszwalb's deformable part model (DPM) [1], a circle de- tector and the JSEG region segmentation. In the second step, we apply a feature-fusion-based food recognition method for bounding boxes of the candidate regions with various kinds of visual features including bag-of-features of SIFT and CSIFT with spatial pyramid (SP-BoF), histogram of oriented gradi- ent (HoG), and Gabor texture features. In the experiments, we estimated ten food candidates for multiple-food images in the descending order of the confi- dence scores. As results, we have achieved the 55.8{\%} classi- fication rate, which improved the baseline result in case of us- ing only DPM by 14.3 points, for a multiple-food image data set. This demonstrates that the proposed two-step method is effective for recognition of multiple-food images.},
author = {Matsuda, Yuji and Hoashi, Hajime and Yanai, Keiji},
booktitle = {Proceedings - IEEE International Conference on Multimedia and Expo},
doi = {10.1109/ICME.2012.157},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Matsuda, Hoashi, Yanai{\_}Proceedings - IEEE International Conference on Multimedia and Expo(2).pdf:pdf},
isbn = {978-1-4673-1659-0},
issn = {19457871},
keywords = {multiple kernel learning,multiple-food image,region detection,window search},
month = {jul},
pages = {25--30},
publisher = {IEEE},
title = {{Recognition of multiple-food images by detecting candidate regions}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6298369},
year = {2012}
}
@article{zhang2015SOD,
author = {Zhang, Jianming and Sclaroff, Stan and Lin, Zhe and Shen, Xiaohui and Price, Brian},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Zhang et al.{\_}IEEE Conference on Computer Vision and Pattern Recognition(CVPR).pdf:pdf},
journal = {IEEE Conference on Computer Vision and Pattern Recognition(CVPR)},
title = {{Unconstrained Salient Object Detection via Proposal Subset Optimization}},
url = {http://cs-people.bu.edu/jmzhang/sod.html},
year = {2016}
}
@inproceedings{Wang2015,
abstract = {This paper deals with automatic systems for image recipe recognition. For this purpose, we compare and evaluate leading vision-based and text-based technologies on a new very large multimodal dataset (UPMC Food-101) containing about 100,000 recipes for a total of 101 food categories. Each item in this dataset is represented by one image plus textual information. We present deep experiments of recipe recognition on our dataset using visual, textual information and fusion. Additionally, we present experiments with text-based embedding technology to represent any food word in a semantical continuous space. We also compare our dataset features with a twin dataset provided by ETHZ university: we revisit their data collection protocols and carry out transfer learning schemes to highlight similarities and differences between both datasets. Finally, we propose a real application for daily users to identify recipes. This application is a web search engine that allows any mobile device to send a query image and retrieve the most relevant recipes in our dataset.},
author = {Wang, Xin and Kumar, Devinder and Thome, Nicolas and Cord, Matthieu and Precioso, Frederic},
booktitle = {2015 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2015},
doi = {10.1109/ICMEW.2015.7169757},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Wang et al.{\_}2015 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2015(2).pdf:pdf},
isbn = {9781479970797},
keywords = {Accuracy,Feature extraction,Google,HTML,Internet,Protocols,Training,UPMC Food-101,Visualization,Web search engine,computer vision,data collection protocols,food categories,food technology,image fusion,image recipe recognition,image recognition,image retrieval,mobile device,multimodal dataset,multimodal food dataset,query image,relevant recipe retrieval,search engines,semantical continuous space,text analysis,text-based embedding technology,text-based technology,transfer learning schemes,vision-based technology},
month = {jun},
pages = {1--6},
publisher = {IEEE},
title = {{Recipe recognition with large multimodal food dataset}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7169757},
year = {2015}
}
@inproceedings{Shimoda2015,
abstract = {We propose a CNN-based food image segmentation which requires no pixel-wise annotation. The proposed method consists of food region proposals by selective search and bounding box clustering, back propagation based saliency map estimation with the CNN model fine-tuned with the UEC-FOOD100 dataset, GrabCut guided by the estimated saliency maps and region integration by non-maximum suppression. In the experiments, the proposed method outperformed RCNN regarding food region detection as well as the PASCAL VOC detection task.},
author = {Shimoda, Wataru and Yanai, Keiji},
booktitle = {New Trends in Image Analysis and Processing -- ICIAP 2015 Workshops},
doi = {10.1007/978-3-319-23222-5_55},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf//Shimoda, Yanai{\_}New Trends in Image Analysis and Processing -- ICIAP 2015 Workshops.pdf:pdf},
isbn = {9783319232218},
issn = {16113349},
keywords = {Convolutional neural network,Deep learning,Food segmentation,UEC-FOOD},
pages = {449--457},
title = {{CNN-based food image segmentation without pixel-wise annotation}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-23222-5{\_}55},
volume = {9281},
year = {2015}
}
@article{Wazumi2013,
author = {Wazumi, Minami and Han, Xian-Hua and Chen, Yen-Wei},
doi = {10.1109/SII.2013.6776730},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Wazumi, Han, Chen{\_}Proceedings of the 2013 IEEESICE International Symposium on System Integration(2).pdf:pdf},
isbn = {978-1-4799-2625-1},
journal = {Proceedings of the 2013 IEEE/SICE International Symposium on System Integration},
keywords = {Educational institutions,Encoding,Feature extraction,Image recognition,Image representation,Sc-based codebook model,Vectors,Visualization,codebook-based mode-bag-of feature,codebook-based model,dietary life,dish recognition,food image recognition,food-log system,health and safety,health care,healthy life,image recognition,image representation,menu content auto-recognition,mobile phone,sparse-coding,spatial pyramid matching,unhealthy diets},
month = {dec},
pages = {482--485},
publisher = {IEEE},
title = {{Food recognition using Codebook-based model with sparse-coding}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6776730},
year = {2013}
}
@inproceedings{Bay2006,
abstract = {Abstract. In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Ro- bust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (in casu, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper presents experimental results on a standard evaluation set, as well as on imagery obtained in the context of a real-life object recognition application. Both show SURF's strong performance.},
author = {Bay, Herbert and Tuytelaars, Tinne and {Van Gool}, Luc},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/11744023_32},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Bay, Tuytelaars, Van Gool{\_}Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture No.pdf:pdf},
isbn = {3540338322},
issn = {03029743},
pages = {404--417},
pmid = {16081019},
title = {{SURF: Speeded up robust features}},
volume = {3951 LNCS},
year = {2006}
}
@article{Nguyen2014,
abstract = {This paper proposes food image classification methods exploiting both local appearance and global structural information of food objects. The contribution of the paper is threefold. First, non-redundant local binary pattern (NRLBP) is used to describe the local appearance information of food objects. Second, the structural information of food objects is represented by the spatial relationship between interest points and encoded using a shape context descriptor formed from those interest points. Third, we propose two methods of integrating appearance and structural information for the description and classification of food images. We evaluated the proposed methods on two datasets. Experimental results verified that the combination of local appearance and structural features can improve classification performance. {\textcopyright} 2014 Elsevier B.V.},
author = {Nguyen, Duc Thanh and Zong, Zhimin and Ogunbona, Philip O. and Probst, Yasmine and Li, Wanqing},
doi = {10.1016/j.neucom.2014.03.017},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Nguyen et al.{\_}Neurocomputing.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Food image classification,Local binary pattern,Non-redundant local binary pattern,Shape context},
pages = {242--251},
title = {{Food image classification using local appearance and global structural information}},
url = {http://www.sciencedirect.com/science/article/pii/S0925231214004317},
volume = {140},
year = {2014}
}
@article{Pouladzadeh2014,
abstract = {As people across the globe are becoming more interested in watching their weight, eating more healthy, and avoiding obesity, a system that can measure calories and nutrition in every day meals can be very useful. In this paper, we propose a food calorie and nutrition measurement system that can help patients and dietitians to measure and manage daily food intake. Our system is built on food image processing and uses nutritional fact tables. Recently, there has been an increase in the usage of personal mobile technology such as smartphones or tablets, which users carry with them practically all the time. Via a special calibration technique, our system uses the built-in camera of such mobile devices and records a photo of the food before and after eating it to measure the consumption of calorie and nutrient components. Our results show that the accuracy of our system is acceptable and it will greatly improve and facilitate current manual calorie measurement techniques.},
author = {Pouladzadeh, Parisa and Shirmohammadi, Shervin and Al-Maghrabi, Rana},
doi = {10.1109/TIM.2014.2303533},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Pouladzadeh, Shirmohammadi, Al-Maghrabi{\_}IEEE Transactions on Instrumentation and Measurement(2).pdf:pdf},
issn = {00189456},
journal = {IEEE Transactions on Instrumentation and Measurement},
keywords = {Calorie measurement,food image processing,obesity management},
month = {aug},
number = {8},
pages = {1947--1956},
publisher = {IEEE},
title = {{Measuring calorie and nutrition from food image}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6748066},
volume = {63},
year = {2014}
}
@misc{Pouladzadeh2014a,
abstract = {As people across the globe are becoming more interested in watching their weight, eating more healthily, and avoiding obesity, a system that can measure calories and nutrition in everyday meals can be very useful. Recently, due to ubiquity of mobile devices such as smart phones, the health monitoring applications are accessible by the patients practically all the time. We have created a semi-automatic food calorie and nutrition measurement system via mobile that can help patients and dietitians to measure and manage daily food intake. While segmentation and recognition are the two main steps of a food calorie measurement system, in this paper we have focused on the recognition part and mainly the training phase of the classification algorithm. This paper presents a cloud-based Support Vector Machine (SVM) method for classifying objects in cluster. We propose a method for food recognition application that is referred to as the Cloud SVM training mechanism in a cloud computing environment with Map Reduce technique for distributed machine learning. The results show that by using cloud computing system in classification phase and updating the database periodically, the accuracy of the recognition step has increased in single food portion, non-mixed and mixed plate of food compared to LIBSVM. {\textcopyright} 2014 Springer Science+Business Media New York.},
author = {Pouladzadeh, Parisa and Shirmohammadi, Shervin and Bakirov, Aslan and Bulut, Ahmet and Yassine, Abdulsalam},
booktitle = {Multimedia Tools and Applications},
doi = {10.1007/s11042-014-2116-x},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Pouladzadeh et al.{\_}Multimedia Tools and Applications.pdf:pdf},
isbn = {13807501},
issn = {13807501},
keywords = {Calorie measurement,Cloud computing,Food image processing},
month = {jul},
number = {14},
pages = {5243--5260},
publisher = {Springer US},
title = {{Cloud-based SVM for food categorization}},
url = {http://link.springer.com/10.1007/s11042-014-2116-x},
volume = {74},
year = {2014}
}
@article{Kawano2013,
abstract = {We propose a mobile food recognition system the poses of which are$\backslash$nestimating calorie and nutritious of foods and recording a user's$\backslash$neating habits. Since all the processes on image recognition performed$\backslash$non a smart-phone, the system does not need to send images to a server$\backslash$nand runs on an ordinary smartphone in a real-time way. To recognize$\backslash$nfood items, a user draws bounding boxes by touching the screen first,$\backslash$nand then the system starts food item recognition within the indicated$\backslash$nbounding boxes. To recognize them more accurately, we segment each$\backslash$nfood item region by GrubCut, extract a color histogram and SURF-based$\backslash$nbag-of-features, and finally classify it into one of the fifty food$\backslash$ncategories with linear SVM and fast 2 kernel. In addition, the system$\backslash$nestimates the direction of food regions where the higher SVM output$\backslash$nscore is expected to be obtained, show it as an arrow on the screen$\backslash$nin order to ask a user to move a smartphone camera. This recognition$\backslash$nprocess is performed repeatedly about once a second. We implemented$\backslash$nthis system as an Android smartphone application so as to use multiple$\backslash$nCPU cores effectively for real-time recognition. In the experiments,$\backslash$nwe have achieved the 81.55{\%} classification rate for the top 5 category$\backslash$ncandidates when the ground-truth bounding boxes are given. In addition,$\backslash$nwe obtained positive evaluation by user study compared to the food$\backslash$nrecording system without object recognition.},
author = {Kawano, Y and Yanai, K},
doi = {10.1109/CVPRW.2013.5},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Kawano, Yanai{\_}Computer Vision and Pattern Recognition Workshops (CVPRW), 2013 IEEE Conference on.pdf:pdf},
isbn = {9780769549903},
issn = {2160-7508},
journal = {Computer Vision and Pattern Recognition Workshops (CVPRW), 2013 IEEE Conference on},
keywords = {behavioural sciences computing;feature extraction;},
pages = {1--7},
title = {{Real-Time Mobile Food Recognition System}},
year = {2013}
}
@inproceedings{Hoashi2010a,
abstract = {Recognition of food images is challenging due to their diversity and practical for health care on foods for people. In this paper, we propose an automatic food image recognition system for 85 food categories by fusing various kinds of image features including bag-of-features{\~{}}(BoF), color histogram, Gabor features and gradient histogram with Multiple Kernel Learning{\~{}}(MKL). In addition, we implemented a prototype system to recognize food images taken by cellular-phone cameras. In the experiment, we have achieved the 62.52{\&}{\#}x025; classification rate for 85 food categories.},
author = {Hoashi, Hajime and Joutou, Taichi and Yanai, Keiji},
booktitle = {Proceedings - 2010 IEEE International Symposium on Multimedia, ISM 2010},
doi = {10.1109/ISM.2010.51},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Hoashi, Joutou, Yanai{\_}Proceedings - 2010 IEEE International Symposium on Multimedia, ISM 2010(2).pdf:pdf},
isbn = {9780769542171},
keywords = {Feature fusion,Food image recognition,Multiple kernel learning},
month = {dec},
pages = {296--301},
publisher = {IEEE},
title = {{Image recognition of 85 food categories by feature fusion}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5693856},
year = {2010}
}
@article{Kawano2014,
abstract = {Abstract In this paper, we report the feature obtained from the Deep Convolutional Neural Network boosts food recognition accuracy greatly by integrating it with conventional hand-crafted image features, Fisher Vectors with HoG and Color patches. In the experiments, we have achieved 72.26{\%} as the top-1 accuracy and 92.00{\%} as the top-5 accuracy for the 100-class food dataset, UEC-FOOD100, which outperforms the best classification accuracy of this dataset reported so far, 59.6{\%}, greatly.},
author = {Kawano, Yoshiyuki and Yanai, Keiji},
doi = {10.1145/2638728.2641339},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Kawano, Yanai{\_}ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp).pdf:pdf},
isbn = {9781450330473},
journal = {ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp)},
keywords = {Author Keywords food recognition,Deep Convolutional Neural Network,Fisher Vector},
pages = {589--593},
title = {{Food Image Recognition with Deep Convolutional Features}},
url = {http://dx.doi.org/10.1145/2638728.2641339},
year = {2014}
}
@article{Arivazhagan2010,
abstract = {The computer vision strategies used to recognize a fruit rely on four basic features which characterize the object: intensity, color, shape and texture. This paper proposes an efficient fusion of color and texture features for fruit recognition. The recognition is done by the minimum distance classifier based upon the statistical and co-occurrence features derived from the Wavelet transformed sub- bands. Experimental results on a database of about 2635 fruits from 15 different classes confirm the effectiveness of the proposed approach.},
author = {Arivazhagan, S and Shebiah, R Newlin and Nidhyanandhan, S Selva and Ganesan, L},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Arivazhagan et al.{\_}Information Sciences.pdf:pdf},
journal = {Information Sciences},
keywords = {co occurrence features,fruit recognition,texture,wavelet transform},
number = {2},
pages = {90--94},
title = {{Fruit Recognition using Color and Texture Features}},
url = {http://cisjournal.org/archive/vol1no1/vol1no1{\_}12.pdf},
volume = {1},
year = {2010}
}
@article{Ristin2014,
abstract = {In recent years, large image data sets such as "ImageNet", "TinyImages" or ever-growing social networks like "Flickr" have emerged, posing new challenges to image classification that were not apparent in smaller image sets. In particular, the efficient handling of dynamically growing data sets, where not only the amount of training images, but also the number of classes increases over time, is a relatively unexplored problem. To remedy this, we introduce Nearest Class Mean Forests (NCMF), a variant of Random Forests where the decision nodes are based on nearest class mean (NCM) classification. NCMFs not only outperform conventional random forests, but are also well suited for integrating new classes. To this end, we propose and compare several approaches to incorporate data from new classes, so as to seamlessly extend the previously trained forest instead of re-training them from scratch. In our experiments, we show that NCMFs trained on small data sets with 10 classes can be extended to large data sets with 1000 classes without significant loss of accuracy compared to training from scratch on the full data.},
author = {Ristin, Marko and Guillaumin, Matthieu and Gall, Juergen and {Van Gool}, Luc},
doi = {10.1109/CVPR.2014.467},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Ristin et al.{\_}Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Incremental learning,image classification,large-scale,nearest class mean classifier,random forest},
month = {mar},
number = {99},
pages = {3654--3661},
publisher = {IEEE},
title = {{Incremental learning of NCM forests for large-scale image classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7164339},
volume = {PP},
year = {2014}
}
@article{Chen2009,
abstract = {We introduce the first visual dataset of fast foods with a total of 4,545 still images, 606 stereo pairs, 303 360° videos for structure from motion, and 27 privacy-preserving videos of eating events of volunteers. This work was motivated by research on fast food recognition for dietary assessment. The data was collected by obtaining three instances of 101 foods from 11 popular fast food chains, and capturing images and videos in both restaurant conditions and a controlled lab setting. We benchmark the dataset using two standard approaches, color histogram and bag of SIFT features in conjunction with a discriminative classifier. Our dataset and the benchmarks are designed to stimulate research in this area and will be released freely to the research community.},
author = {Chen, Mei and Dhingra, Kapil and Wu, Wen and Yang, Lei and Sukthankar, Rahul and Yang, Jie},
doi = {10.1109/ICIP.2009.5413511},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Chen et al.{\_}Proceedings - International Conference on Image Processing, ICIP.pdf:pdf},
isbn = {9781424456543},
issn = {15224880},
journal = {Proceedings - International Conference on Image Processing, ICIP},
keywords = {Food image dataset,Object recognition},
pages = {289--292},
title = {{PFID: Pittsburgh Fast-food Image Dataset}},
year = {2009}
}
@article{Pouladzadeh2015a,
abstract = {The integration of multimedia-assisted healthcare systems with could-computing services and mobile technologies has led to increased accessibility for healthcare providers and patients. Utilizing cloud computing infrastructures and virtualization technologies allows for the transformation of traditional healthcare systems that demand manual care and monitoring to more salient, automatic and cost effective systems. The goal of this paper is to develop a multimedia-assisted mobile healthcare application using cloud-computing virtualization technologies. We consider calorie measurement as an example healthcare application that can benefit from cloud-computing virtualization technology. The key functionalities of our application entail image segmentation, image processing and deep learning algorithms for food classification and recognition. Client side devices (e.g. smartphones, tablets etc.) have limitations in handling time sensitive and computationally intensive algorithms pertained to our application. Image processing and deep learning algorithms, used in food recognition and calorie measurement, consume devices' batteries quickly, which is inconvenient for the user. It is also very challenging for client side devices to scale for large number of data and images, as needed for food recognition. The entire process is time-consuming and inefficient and discomforting from users' perspective and may deter them from using the application. In this paper, we address these challenges by proposing a virtualization mechanism in cloud computing that utilizes the Android architecture. Android allows for parting an application into activities run by the front-end user and services run by the back-end tasks. In the proposed virtualization mechanism, we use both the hosted and the hypervisor models to publish our Android-based food recognition and calorie measurement application in the cloud. By so doing, the users of our application can control their virtual smartphone operations through a dedicated client application installed on their smartphones, while the processing of the application continue to run on the virtual Android image even if the user is disconnected due to any unexpected event. We have performed several experiments to validate our mechanism. Specifically, we have run our deep learning and image processing algorithms for food recognition on different configuration platforms on both the cloud and local server connected to the mobile. The results show that the accuracy of the system with the virtualization mechanism is more than 94.33 {\%} compared to 87.16 {\%} when we run the application locally. Also, with our virtualization mechanism the results are processed 49 {\%} faster than the case of running the application locally.},
author = {Pouladzadeh, Parisa and Peddi, Sri Vijay Bharat and Kuhad, Pallavi and Yassine, Abdulsalam and Shirmohammadi, Shervin},
doi = {10.1007/s10586-015-0468-2},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf//Pouladzadeh et al.{\_}Cluster Computing.pdf:pdf},
isbn = {1058601504},
issn = {15737543},
journal = {Cluster Computing},
keywords = {Cloud computing,Food recognition,Healthcare,Multimedia,Virtualization},
month = {sep},
number = {3},
pages = {1099--1110},
publisher = {Springer US},
title = {{A virtualization mechanism for real-time multimedia-assisted mobile food recognition application in cloud computing}},
url = {http://link.springer.com/10.1007/s10586-015-0468-2},
volume = {18},
year = {2015}
}
@article{Wazumi2011,
abstract = {Email Print Request Permissions Recently, with the increasing of unhealthy diets and the attracted attention for healthy life, how to manage the dietary life is becoming more and more important. In this paper, we aim to construct a system, which can auto-recognize the menu contents from food image taken by mobile phone. As we know that the viewpoints can be varied in any direction when taking food images, and then, rotation-robust features for image representation are very important. Therefore, in this paper, we propose to extract rotation invariant features using circle-segmentation called SPIN for food recognition, and construct a Food-Log system, which records the contents of food menu, calories and nutritional value for management of the dietary life.},
author = {Wazumi, Minami and Han, Xian-hua and Ai, Danni and Chen, Yen-wei},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Wazumi et al.{\_}Computer Sciences and Convergence Information Technology (ICCIT), 2011 6th International Conference on.pdf:pdf},
isbn = {9788988678541},
journal = {Computer Sciences and Convergence Information Technology (ICCIT), 2011 6th International Conference on},
pages = {874 -- 877},
title = {{Auto-Recognition of Food Images Using SPIN Feature for Food-Log System}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6316741},
year = {2011}
}
@inproceedings{Bossard2014,
abstract = {In this paper we address the problem of automatically rec- ognizing pictured dishes. To this end, we introduce a novel method to mine discriminative parts using Random Forests (rf), which allows us to mine for parts simultaneously for all classes and to share knowledge among them. To improve efficiency of mining and classification, we only consider patches that are aligned with image superpixels, which we call components. To measure the performance of our rf component mining for food recognition, we introduce a novel and challenging dataset of 101 food categories, with 101'000 images. With an average accuracy of 50.76{\%}, our model outperforms alternative classification methods except for cnn, including svm classification on Improved Fisher Vectors and existing discriminative part-mining algorithms by 11.88{\%} and 8.13{\%}, re- spectively. On the challenging mit-Indoor dataset, our method compares nicely to other s-o-a component-based classification methods.},
archivePrefix = {arXiv},
arxivId = {10.1007/978-3-319-10599-4{\_}29},
author = {Bossard, Lukas and Guillaumin, Matthieu and {Van Gool}, Luc},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10599-4_29},
eprint = {978-3-319-10599-4{\_}29},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Bossard, Guillaumin, Van Gool{\_}Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lectur.pdf:pdf},
isbn = {9783319105987},
issn = {16113349},
keywords = {Discriminative part mining,Food recognition,Image classification,Random Forest},
number = {PART 6},
pages = {446--461},
primaryClass = {10.1007},
title = {{Food-101 - Mining discriminative components with random forests}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-10599-4{\_}29},
volume = {8694 LNCS},
year = {2014}
}
@inproceedings{Yang2010,
abstract = {Food recognition is difficult because food items are de-formable objects that exhibit significant variations in appearance. We believe the key to recognizing food is to exploit the spatial relationships between different ingredients (such as meat and bread in a sandwich). We propose a new representation for food items that calculates pairwise statistics between local features computed over a soft pixel-level segmentation of the image into eight ingredient types. We accumulate these statistics in a multi-dimensional histogram, which is then used as a feature vector for a discriminative classifier. Our experiments show that the proposed representation is significantly more accurate at identifying food than existing methods.},
author = {Yang, Shulin and Chen, Mei and Pomerleau, Dean and Sukthankar, Rahul},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5539907},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Yang et al.{\_}Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition(2).pdf:pdf},
isbn = {9781424469840},
issn = {10636919},
keywords = {Computer vision,Histograms,Image edge detection,Image segmentation,Object recognition,Pixel,Predictive models,Robots,Shape,Statistics,discriminative classifier,feature vector,food recognition,image segmentation,local features,multi-dimensional histogram,object recognition,pairwise statistics,soft pixel-level segmentation},
month = {jun},
pages = {2249--2256},
publisher = {IEEE},
title = {{Food recognition using statistics of pairwise local features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5539907},
year = {2010}
}
@article{Chen2012,
abstract = {Computer-aided food identification and quantity estimation have caught more attention than before due to the growing concern of health and obesity. The identification problem is usually defined as an image categorization or classification problem and several researches on this topic have been proposed. In this paper, we address the issues of feature descriptors in the food identification problem and introduce a preliminary approach for the quantity es- timation using depth information. Sparse coding is utilized in the SIFT and Local binary pattern feature descriptors, and these fea- tures combined with Gabor and color features are used to represent food items. A multi-label SVM classifier is trained for each fea- ture, and these classifiers are combined with multi-class Adaboost algorithm. For evaluation, 50 major categories of worldwide food are used, and each category contains 100 photographs from differ- ent sources, such as photos taken manually or from Internet web albums. An overall accuracy of 68.3{\%} is achieved, and success at top-N candidates achieved 80.6{\%}, 84.8{\%}, and 90.9{\%} accuracy accordingly when N equals 2, 3, and 5, thus making mobile appli- cation practical. The experimental results show that the proposed methods greatly improve the performance of original SIFT and LBP feature descriptors. On the other hand, for quantity estimation us- ing depth information, a straight forward method is proposed for certain food, while transparent food ingredients such as pure water and},
author = {Chen, Mei-Yun and Yang, Yung-Hsiang and Ho, Chia-Ju and Wang, Shih-Han and Liu, Shane-Ming and Chang, Eugene and Yeh, Che-Hua and Ouhyoung, Ming},
doi = {10.1145/2407746.2407775},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Chen et al.{\_}SIGGRAPH Asia.pdf:pdf},
isbn = {9781450319157},
journal = {SIGGRAPH Asia},
pages = {1--4},
title = {{Automatic Chinese food identification and quantity estimation}},
url = {http://dl.acm.org/citation.cfm?doid=2407746.2407775},
year = {2012}
}
@inproceedings{Beijbom2015,
abstract = {Logging food and calorie intake has been shown to facilitate weight management. Unfortunately, current food logging methods are time-consuming and cumbersome, which limits their effectiveness. To address this limitation, we present an automated computer vision system for logging food and calorie intake using images. We focus on the "restaurant" scenario, which is often a challenging aspect of diet management. We introduce a key insight that addresses this problem specifically: restaurant plates are often both nutritionally and visually consistent across many servings. This insight provides a path to robust calorie estimation from a single RGB photograph: using a database of known food items together with restaurant-specific classifiers, calorie estimation can be achieved through identification followed by calorie lookup. As demonstrated on a challenging Menu-Match dataset and an existing third party dataset, our approach outperforms previous computer vision methods and a commercial calorie estimation app. Our Menu-Match dataset of realistic restaurant meals is made publicly available.},
author = {Beijbom, Oscar and Joshi, Neel and Morris, Dan and Saponas, Scott and Khullar, Siddharth},
booktitle = {Proceedings - 2015 IEEE Winter Conference on Applications of Computer Vision, WACV 2015},
doi = {10.1109/WACV.2015.117},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Beijbom et al.{\_}Proceedings - 2015 IEEE Winter Conference on Applications of Computer Vision, WACV 2015(2).pdf:pdf},
isbn = {9781479966820},
keywords = {Computer vision,Databases,Estimation,Feature extraction,Image color analysis,Menu-Match dataset,Standards,Visualization,automated computer vision system,calorie estimation,calorie intake logging,catering industry,computer vision,diet management,image classification,image colour analysis,restaurant meals,restaurant plates,restaurant-specific classifiers,restaurant-specific food logging,single RGB photograph},
month = {jan},
pages = {844--851},
publisher = {IEEE},
title = {{Menu-match: Restaurant-specific food logging from images}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7045971},
year = {2015}
}
@inproceedings{Kusumoto2013,
author = {Kusumoto, Riko and Han, Xian Hua and Chen, Yen Wei},
booktitle = {Proceedings of the 2013 6th International Conference on Biomedical Engineering and Informatics, BMEI 2013},
doi = {10.1109/BMEI.2013.6747060},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Kusumoto, Han, Chen{\_}Proceedings of the 2013 6th International Conference on Biomedical Engineering and Informatics, BMEI 2013(2).pdf:pdf},
isbn = {978-1-4799-2761-6},
keywords = {food,formatting,image recognation,sparse coding},
month = {dec},
pages = {851--855},
publisher = {IEEE},
title = {{Sparse model in hierarchic spatial structure for food image recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6747060},
year = {2013}
}
@article{Thendral2014a,
abstract = {In this paper, we presented two segmentation methods. Edge based and color based detection methods were used to segment images of orange fruits obtained under natural lighting conditions. Twenty digitized images of orange fruits were randomly selected from the Internet in order to find an orange in each image and to determine its location. We compared the results of both segmentation results and the color based segmentation outperforms the edge based segmentation in all aspects. The MATLAB image processing toolbox is used for the computation and comparison results are shown in the segmented image results.},
author = {Thendral, R. and Suhasini, A. and Senthil, N.},
doi = {10.1109/ICCSP.2014.6949884},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Thendral, Suhasini, Senthil{\_}International Conference on Communication and Signal Processing, ICCSP 2014 - Proceedings.pdf:pdf},
isbn = {9781479933587},
journal = {International Conference on Communication and Signal Processing, ICCSP 2014 - Proceedings},
keywords = {Orange harvesting,color based segmentation,edge based segmentation,machine vision},
pages = {463--466},
title = {{A comparative analysis of edge and color based segmentation for orange fruit recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6949884},
year = {2014}
}
@article{Kawano2014a,
abstract = {We propose a mobile food recognition system, FoodCam, the purposes of which are estimating calorie and nutrition of foods and recording a user's eating habits. In this paper, we propose image recognition methods which are suitable for mobile devices. The proposed method enables real-time food image recognition on a consumer smartphone. This characteristic is completely different from the existing systems which require to send images to an image recognition server. To recognize food items, a user draws bounding boxes by touching the screen first, and then the system starts food item recognition within the indicated bounding boxes. To recognize them more accurately, we segment each food item region by GrubCut, extract image features and finally classify it into one of the one hundred food categories with a linear SVM. As image features, we adopt two kinds of features: one is the combination of the standard bag-of-features and color histograms with $\chi$2 kernel feature maps, and the other is a HOG patch descriptor and a color patch descriptor with the state-of-the-art Fisher Vector representation. In addition, the system estimates the direction of food regions where the higher SVM output score is expected to be obtained, and it shows the estimated direction in an arrow on the screen in order to ask a user to move a smartphone camera. This recognition process is performed repeatedly and continuously. We implemented this system as a standalone mobile application for Android smartphones so as to use multiple CPU cores effectively for real-time recognition. In the experiments, we have achieved the 79.2 {\%} classification rate for the top 5 category candidates for a 100-category food dataset with the ground-truth bounding boxes when we used HOG and color patches with the Fisher Vector coding as image features. In addition, we obtained positive evaluation by a user study compared to the food recording system without object recognition. {\textcopyright} 2014 Springer Science+Business Media New York.},
author = {Kawano, Yoshiyuki and Yanai, Keiji},
doi = {10.1007/s11042-014-2000-8},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Kawano, Yanai{\_}Multimedia Tools and Applications.pdf:pdf},
isbn = {13807501},
issn = {13807501},
journal = {Multimedia Tools and Applications},
keywords = {Dietary recording,Fisher vector,Food recognition,Mobile image recognition,Smartphone application},
pages = {5263--5287},
title = {{FoodCam: A real-time food recognition system on a smartphone}},
year = {2014}
}
@article{Jia2014,
abstract = {OBJECTIVE: Accurate estimation of food portion size is of paramount importance in dietary studies. We have developed a small, chest-worn electronic device called eButton which automatically takes pictures of consumed foods for objective dietary assessment. From the acquired pictures, the food portion size can be calculated semi-automatically with the help of computer software. The aim of the present study is to evaluate the accuracy of the calculated food portion size (volumes) from eButton pictures.$\backslash$n$\backslash$nDESIGN: Participants wore an eButton during their lunch. The volume of food in each eButton picture was calculated using software. For comparison, three raters estimated the food volume by viewing the same picture. The actual volume was determined by physical measurement using seed displacement.$\backslash$n$\backslash$nSETTING: Dining room and offices in a research laboratory.$\backslash$n$\backslash$nSUBJECTS: Seven lab member volunteers.$\backslash$n$\backslash$nRESULTS: Images of 100 food samples (fifty Western and fifty Asian foods) were collected and each food volume was estimated from these images using software. The mean relative error between the estimated volume and the actual volume over all the samples was -2{\textperiodcentered}8 {\%} (95 {\%} CI -6{\textperiodcentered}8 {\%}, 1{\textperiodcentered}2 {\%}) with sd of 20{\textperiodcentered}4 {\%}. For eighty-five samples, the food volumes determined by computer differed by no more than 30 {\%} from the results of actual physical measurements. When the volume estimates by the computer and raters were compared, the computer estimates showed much less bias and variability.$\backslash$n$\backslash$nCONCLUSIONS: From the same eButton pictures, the computer-based method provides more objective and accurate estimates of food volume than the visual estimation method.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Jia, Wenyan and Chen, Hsin-Chen and Yue, Yaofeng and Li, Zhaoxin and Fernstrom, John and Bai, Yicheng and Li, Chengliu and Sun, Mingui},
doi = {10.1017/S1368980013003236},
eprint = {NIHMS150003},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Jia et al.{\_}Public health nutrition.pdf:pdf},
isbn = {2122633255},
issn = {1475-2727},
journal = {Public health nutrition},
keywords = {Adult,Diet Surveys,Energy Intake,Female,Food,Humans,Lunch,Male,Photography,Portion Size,Regression Analysis,Reproducibility of Results,Size Perception,Thorax},
number = {8},
pages = {1671--81},
pmid = {24476848},
title = {{Accuracy of food portion size estimation from digital pictures acquired by a chest-worn camera.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4152011{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {17},
year = {2014}
}
@article{Almaghrabi2012a,
abstract = {In this paper, a food nutrition and energy intake recognition system for medical purposes is proposed. This system is built based on food image processing and shape recognition in addition to nutritional fact tables. Recently, countless studies suggested that the usage of technology such as smartphones may enhance the treatments for obesity and overweight patients. Via a special technique, the system records a photo of the food before and after eating in order to estimate the consumption calorie of the selected food and its nutrients components. Our system presents a new instrument in food intake measuring systems which can be useful and effective in obesity management.},
author = {Almaghrabi, Rana and Villalobos, Gregorio and Pouladzadeh, Parisa and Shirmohammadi, Shervin},
doi = {10.1109/I2MTC.2012.6229581},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Almaghrabi et al.{\_}2012 Ieee I2Mtc.pdf:pdf},
isbn = {9781457717710},
issn = {1091-5281},
journal = {2012 Ieee I2Mtc},
keywords = {Calories measurement,Image processing,Shape recognition,obesity management},
pages = {366--370},
title = {{A novel method for measuring nutrition intake based on food image}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6229581},
year = {2012}
}
@inproceedings{Martinel2015,
abstract = {Food recognition is an emerging computer vision topic. The problem is characterized by the absence of rigid struc-ture of the food and by the large intra-class variations. Ex-isting approaches tackle the problem by designing ad-hoc feature representations based on a priori knowledge of the problem. Differently from these, we propose a committee-based recognition system that chooses the optimal features out of the existing plethora of available ones (e.g., color, texture, etc.). Each committee member is an Extreme Learn-ing Machine trained to classify food plates on the basis of a single feature type. Single member classifications are then considered by a structural Support Vector Machine to produce the final ranking of possible matches. This is achieved by filtering out the irrelevant features/classifiers, thus considering only the relevant ones. Experimental re-sults show that the proposed system outperforms state-of-the-art works on the most used three publicly available benchmark datasets.},
author = {Martinel, Niki and Piciarelli, Claudio and Micheloni, Christian and Foresti, Gian Luca},
booktitle = {International Conference on Computer Vision Workshops},
doi = {10.1109/ICCVW.2015.70},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Martinel et al.{\_}International Conference on Computer Vision Workshops.pdf:pdf},
isbn = {9780769557205},
title = {{A Structured Committee for Food Recognition}},
year = {2015}
}
@article{Pishva2000,
author = {Pishva, D. and Hirakawa, K. and Kawai, A. and Shiino, T.},
doi = {10.1109/ICOSP.2000.891642},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Pishva et al.{\_}WCC 2000 - ICSP 2000. 2000 5th International Conference on Signal Processing Proceedings. 16th World Computer Congress (2).pdf:pdf},
isbn = {0-7803-5747-7},
journal = {WCC 2000 - ICSP 2000. 2000 5th International Conference on Signal Processing Proceedings. 16th World Computer Congress 2000},
keywords = {bread,cluster analysis,color,color algorithms assume a,could have any,distribution,fixed facial orientation,machine vision,orientation,our application bread samples,since in,texture,we developed an indigenous},
pages = {840--844},
publisher = {IEEE},
title = {{A unified image segmentation approach with application to bread recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=891642},
volume = {2},
year = {2000}
}
@article{Anthimopoulos2014,
abstract = {Computer vision-based food recognition could be used to estimate a meal's carbohydrate content for diabetic pa- tients.This study proposes amethodology for automatic foodrecog- nition, based on the bag-of-features (BoF) model. An extensive technical investigation was conducted for the identification and optimization of the best performing components involved in the BoF architecture, as well as the estimation of the corresponding parameters. For the design and evaluation of the prototype sys- tem, a visual dataset with nearly 5000 food imageswas created and organized into 11 classes. The optimized system computes dense local features, using the scale-invariant feature transform on the HSV color space, builds a visual dictionary of 10000 visual words by using the hierarchical k-means clustering and finally classifies the food images with a linear support vector machine classifier. The system achieved classification accuracy of the order of 78{\%}, thus proving the feasibility of the proposed approach in a very challenging image dataset. Index},
author = {Anthimopoulos, Marios M. and Gianola, Lauro and Scarnato, Luca and Diem, Peter and Mougiakakou, Stavroula G.},
doi = {10.1109/JBHI.2014.2308928},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Anthimopoulos et al.{\_}IEEE Journal of Biomedical and Health Informatics(2).pdf:pdf},
issn = {21682194},
journal = {IEEE Journal of Biomedical and Health Informatics},
keywords = {Bag of features (BoF),diabetes,feature extraction,food recognition,image classification},
month = {jul},
number = {4},
pages = {1261--1271},
pmid = {25014934},
publisher = {IEEE},
title = {{A food recognition system for diabetic patients based on an optimized bag-of-features model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6762879},
volume = {18},
year = {2014}
}
@inproceedings{Kagaya2014,
abstract = {In this paper, we apply a convolutional neural network (CNN) to the tasks of detecting and recognizing food images. Be-cause of the wide diversity of types of food, image recog-nition of food items is generally very difficult. However, deep learning has been shown recently to be a very powerful image recognition technique, and CNN is a state-of-the-art approach to deep learning. We applied CNN to the tasks of food detection and recognition through parameter optimiza-tion. We constructed a dataset of the most frequent food items in a publicly available food-logging system, and used it to evaluate recognition performance. CNN showed signif-icantly higher accuracy than did traditional support-vector-machine-based methods with handcrafted features. In addi-tion, we found that the convolution kernels show that color dominates the feature extraction process. For food image detection, CNN also showed significantly higher accuracy than a conventional method did.},
author = {Kagaya, Hokuto and Aizawa, Kiyoharu and Ogawa, Makoto},
booktitle = {ACM Multimedia},
doi = {10.1145/2647868.2654970},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf//Kagaya, Aizawa, Ogawa{\_}ACM Multimedia.pdf:pdf},
isbn = {9781450330633},
keywords = {convolu-,deep learning,food detection,food recognition,tional neural network},
number = {2},
pages = {1085--1088},
title = {{Food Detection and Recognition Using Convolutional Neural Network}},
url = {http://dl.acm.org/citation.cfm?doid=2647868.2654970},
year = {2014}
}
@article{ParisaPouladzadehAbdulsalamYassine2015,
author = {{Abdulsalam Yassine}, Parisa Pouladzadeh and {Shervin Shirmohammadi}},
doi = {10.1007/978-3-319-23222-5},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Abdulsalam Yassine, Shervin Shirmohammadi{\_}New Trends in Image Analysis and Processing -- ICIAP 2015 Workshops.pdf:pdf},
isbn = {9783319232218},
issn = {16113349},
journal = {New Trends in Image Analysis and Processing -- ICIAP 2015 Workshops},
keywords = {calorie measurement,food detection,food image dataset},
pages = {441--448},
title = {{FooDD: Food Detection Dataset for Calorie Measurement Using Food Images}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-23222-5{\_}54},
volume = {9281},
year = {2015}
}
@article{Kitamura2008,
abstract = {In this paper, a food-logging system that can distinguish food images from other images, analyze the food balance, and visualize the log is presented. The image processing is based on feature vectors consisting of color histograms, DCT coefficients, detected image patterns and so forth. Support Vector Machine (SVM) was used to detect food images and to analyze the food balance. Experimental results show that the food image extraction presents above 88{\%} of accuray and the food balance estimation is achieved with more than 73{\%} of accuracy.},
author = {Kitamura, Keigo and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
doi = {10.1145/1459359.1459548},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Kitamura, Yamasaki, Aizawa{\_}ACM international conference on Multimedia.pdf:pdf},
isbn = {9781605583037},
journal = {ACM international conference on Multimedia},
pages = {999},
title = {{Food log by analyzing food images}},
url = {http://portal.acm.org/citation.cfm?doid=1459359.1459548},
year = {2008}
}
@article{Hu1962,
abstract = {In this paper a theory of two-dimensional moment invariants for planar geometric figures is presented. A fundamental theorem is established to relate such moment invariants to the well-known algebraic invariants. Complete systems of moment invariants under translation, similitude and orthogonal transformations are derived. Some moment invariants under general two-dimensional linear transformations are also included. Both theoretical formulation and practical models of visual pattern recognition based upon these moment invariants are discussed. A simple simulation program together with its performance are also presented. It is shown that recognition of geometrical patterns and alphabetical characters independently of position, size and orientation can be accomplished. It is also indicated that generalization is possible to include invariance with parallel projection.},
author = {Hu, Ming-Kuei},
doi = {10.1109/TIT.1962.1057692},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Hu{\_}IRE Transactions on Information Theory.pdf:pdf},
isbn = {0096-1000},
issn = {0096-1000},
journal = {IRE Transactions on Information Theory},
keywords = {Artificial intelligence,Bibliographies,Character recognition,Decision theory,Distribution functions,Image analysis,Information processing,Information theory,Pattern recognition,Senior members,Shape},
pages = {179--187},
title = {{Visual pattern recognition by moment invariants}},
volume = {8},
year = {1962}
}
@inproceedings{DeSilva2011,
abstract = {Food images have been receiving increased attention in recent dietary control methods. We present the current status of our web-based system that can be used as a dietary management support system by ordinary Internet users. The system analyzes image archives of the user to identify images of meals. Further image analysis determines the nutritional composition of these meals and stores the data to form a Foodlog. The user can view the data in different formats, and also edit the data to correct any mistakes that occurred during image analysis. This paper presents detailed analysis of the performance of the current system and proposes an improvement of analysis by pre-classification and personalization. As a result, the accuracy of food balance estimation is significantly improved.},
author = {{De Silva}, Gamhewage C. and Aizawa, Kiyoharu},
booktitle = {Proceedings - IEEE International Conference on Multimedia and Expo},
doi = {10.1109/ICME.2011.6012167},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/De Silva, Aizawa{\_}Proceedings - IEEE International Conference on Multimedia and Expo(2).pdf:pdf},
isbn = {9781612843490},
issn = {19457871},
keywords = {FoodLog,Meal summary,clustering,meal image analysis,segmentation},
month = {jul},
pages = {1--6},
publisher = {IEEE},
title = {{Clustering meal images in a web-based dietary management system}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6012167},
year = {2011}
}
@article{Noronha2011,
abstract = {We introduce PlateMate, a system that allows users to take photos of their meals and receive estimates of food intake and composition. Accurate awareness of this information can help people monitor their progress towards dieting goals, but current methods for food logging via self-reporting, expert observation, or algorithmic analysis are time-consuming, expensive, or inaccurate. PlateMate crowdsources nutritional analysis from photographs using Amazon Mechanical Turk, automatically coordinating untrained workers to estimate a meal's calories, fat, carbohydrates, and protein. We present the Management framework for crowdsourcing complex tasks, which supports PlateMate's nutrition analysis workflow. Results of our evaluations show that PlateMate is nearly as accurate as a trained dietitian and easier to use for most users than traditional self-reporting.},
author = {Noronha, Jon and Hysen, Eric and Zhang, Haoqi and Gajos, Krzysztof Z},
doi = {10.1145/2047196.2047198},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Noronha et al.{\_}Proceedings of the 24th annual ACM symposium on User interface software and technology - UIST '11.pdf:pdf},
isbn = {9781450307161},
journal = {Proceedings of the 24th annual ACM symposium on User interface software and technology - UIST '11},
keywords = {crowdsourcing,human computation,ical turk,mechan-,nutrition,remote food photography},
pages = {1},
title = {{Platemate: crowdsourcing nutritional analysis from food photographs}},
url = {http://dl.acm.org/citation.cfm?doid=2047196.2047198},
year = {2011}
}
@article{Pishva2000a,
abstract = {We describe a unique shape based segmentation and color distribution analysis approach that can be used in a machine vision based cash register system for commodity pricing of hand made breads. Systematic sorting and classification of bread samples according to their shapes, sizes, textures and surface color distribution is explored. In this paper, we extracted true object color from bread sample images and developed our analytical approach based on how humans usually distinguish one object from another. We also considered the similarities and differences that exist between this application and other image analysis applications such as hand written Chinese character recognition and human face detection methods. Because of the diversity of our samples, some identification procedures were just shape and size analyses while other samples required textural analysis, color and surface color distribution analyses.},
author = {Pishva, D. and Hirakawa, K. and Kawai, A. and Shiino, T.},
doi = {10.1109/ICOSP.2000.891642},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Pishva et al.{\_}WCC 2000 - ICSP 2000. 2000 5th International Conference on Signal Processing Proceedings. 16th World Computer Congress (3).pdf:pdf},
isbn = {0-7803-5747-7},
journal = {WCC 2000 - ICSP 2000. 2000 5th International Conference on Signal Processing Proceedings. 16th World Computer Congress 2000},
keywords = {bread,cluster analysis,color,color algorithms assume a,could have any,distribution,fixed facial orientation,machine vision,orientation,our application bread samples,since in,texture,we developed an indigenous},
pages = {840--844},
title = {{Shape Based Segmentation and Color Distribution Analysis with Application to Bread Recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=891642},
volume = {2},
year = {2000}
}
@article{Kitamura2009,
abstract = {With the increase of the number of food images on the Internet, we have been developing a food-logging system which has an automated analysis function as a Web application. It can distinguish food images from other images, analyze the food balance, and visualize the log. In this paper, we demonstrate how the performance can be improved by the personalized models. Because our Web application has an interface to review and correct the food analysis results, the generation of the personalized models can be done on-line. Experimental results using two hundred images showed that the extracted image feature vectors differ from user to user but on the other hand the feature vectors and the food balance of each user have a strong correlation. Therefore, the accuracy of the food balance estimation was improved from 37{\%} to 42{\%} on average by the personalized classifier. Copyright 2009 ACM.},
author = {Kitamura, Keigo and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
doi = {10.1145/1630995.1631001},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Kitamura, Yamasaki, Aizawa{\_}Proceedings of the ACM multimedia 2009 workshop on Multimedia for cooking and eating activities - CEA '09.pdf:pdf},
isbn = {9781605587639},
journal = {Proceedings of the ACM multimedia 2009 workshop on Multimedia for cooking and eating activities - CEA '09},
keywords = {Food,Life-log,Multimedia interfaces},
pages = {23},
title = {{FoodLog: Capture, Analysis and Retrieval of Personal Food Images via Web}},
url = {http://portal.acm.org/citation.cfm?doid=1630995.1631001},
year = {2009}
}
@article{Aizawa2013,
author = {Aizawa, Kiyoharu and Maruyama, Yuto and Li, He and Morikawa, Chamin and {De Silva}, G. C.},
doi = {10.1109/TMM.2013.2271474},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Aizawa et al.{\_}IEEE Transactions on Multimedia(2).pdf:pdf},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Bayesian estimation,Food log,Food record,Image processing,Lifelog,Multimedia},
month = {dec},
number = {8},
pages = {2176--2185},
publisher = {IEEE},
title = {{Food balance estimation by using personal dietary tendencies in a multimedia food log}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6548059},
volume = {15},
year = {2013}
}
@article{Vedaldi2010,
author = {Vedaldi, A and Zisserman, A},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Vedaldi, Zisserman{\_}{\{}IEEE{\}} Int. Conf. on Computer Vision and Pattern Recognition.pdf:pdf},
isbn = {9781424469857},
journal = {{\{}IEEE{\}} Int. Conf. on Computer Vision and Pattern Recognition},
number = {Xx},
pages = {3539--3546},
title = {{Efficient Additive Kernels via Explicit Feature Maps}},
volume = {XX},
year = {2010}
}
@article{Yao2012,
abstract = {Fine-grained categorization refers to the task of classifying objects that belong to the same basic-level class (e.g. different bird species) and share similar shape or visual appearances. Most of the state-of-the-art basic-level object classification algorithms have difficulties in this challenging problem. One reason for this can be attributed to the popular codebook-based image representation, often resulting in loss of subtle image information that are critical for fine-grained classification. Another way to address this problem is to introduce human annotations of object attributes or key points, a tedious process that is also difficult to generalize to new tasks. In this work, we propose a codebook-free and annotation-free approach for fine-grained image categorization. Instead of using vector-quantized codewords, we obtain an image representation by running a high throughput template matching process using a large number of randomly generated image templates. We then propose a novel bagging-based algorithm to build a final classifier by aggregating a set of discriminative yet largely uncorrelated classifiers. Experimental results show that our method outperforms state-of-the-art classification approaches on the Caltech-UCSD Birds dataset.},
author = {Yao, Bangpeng and Bradski, Gary and Fei-Fei, Li},
doi = {10.1109/CVPR.2012.6248088},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Yao, Bradski, Fei-Fei{\_}Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3466--3473},
title = {{A codebook-free and annotation-free approach for fine-grained image categorization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6248088},
year = {2012}
}
@article{Christodoulidis2015,
author = {Christodoulidis, Stergios and Anthimopoulos, Marios},
doi = {10.1007/978-3-319-23222-5},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf//Christodoulidis, Anthimopoulos{\_}New Trends in Image Analysis and Processing -- ICIAP 2015 Workshops.pdf:pdf},
isbn = {978-3-319-23221-8},
journal = {New Trends in Image Analysis and Processing -- ICIAP 2015 Workshops},
keywords = {agement,convolutional neural networks,dietary man-,food recognition,machine learning},
pages = {458--465},
publisher = {Springer International Publishing},
title = {{Food Recognition for Dietary Assessment Using Deep Convolutional Neural Networks Stergios}},
url = {http://link.springer.com/10.1007/978-3-319-23222-5{\_}56 http://link.springer.com/10.1007/978-3-319-23222-5},
volume = {9281},
year = {2015}
}
@inproceedings{Ao2015,
author = {Ao, Shuang and Ling, Charles X.},
booktitle = {2015 IEEE International Conference on Data Mining Workshop (ICDMW)},
doi = {10.1109/ICDMW.2015.203},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Ao, Ling{\_}2015 IEEE International Conference on Data Mining Workshop (ICDMW)(2).pdf:pdf},
isbn = {978-1-4673-8493-3},
keywords = {Adaptation models,Computer science,Conferences,Data mining,Feature extraction,GoogLeNet,Image recognition,Training,data mining,deep representation,feature extraction,feature extractor,feature representation,food image recognition,food processing industry,food products,image recognition,image representation,negative classifier,pattern classification},
month = {nov},
pages = {1196--1203},
publisher = {IEEE},
title = {{Adapting New Categories for Food Recognition with Deep Representation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7395804},
year = {2015}
}
@article{Wen2009,
abstract = {Accurate and passive acquisition of dietary data from patients is essential for a better understanding of the etiology of obesity and development of effective weight management programs. Self-reporting is currently the main method for such data acquisition. However, studies have shown that data obtained by self-reporting seriously underestimate food intake and thus do not accurately reflect the real habitual behavior of individuals. Computer food recognition programs have not yet been developed. In this paper, we present a study for recognizing foods from videos of eating, which are directly recorded in restaurants by a web camera. From recognition results, our method then estimates food calories of intake. We have evaluated our method on a database of 101 foods from 9 food restaurants in USA and obtained promising results.},
author = {Wen, Wu and Jie, Yang},
doi = {10.1109/ICME.2009.5202718},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Wen, Jie{\_}Proceedings - 2009 IEEE International Conference on Multimedia and Expo, ICME 2009.pdf:pdf},
isbn = {9781424442911},
issn = {1945-7871},
journal = {Proceedings - 2009 IEEE International Conference on Multimedia and Expo, ICME 2009},
keywords = {Calorie estimation,Fast food recognition},
pages = {1210--1213},
title = {{Fast food recognition from videos of eating for calorie estimation}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5202718},
year = {2009}
}
@inproceedings{Farinella2014,
abstract = {The classification of food images is an interesting and challenging problem since the high variability of the image content which makes the task difficult for current state-of-the-art classification methods. The image representation to be employed in the classification engine plays an important role. We believe that texture features have been not properly considered in this application domain. This paper points out, through a set of experiments, that textures are fundamental to properly recognize different food items. For this purpose the bag of visual words model (BoW) is employed. Images are processed with a bank of rotation and scale invariant filters and then a small codebook of Textons is built for each food class. The learned class-based Textons are hence collected in a single visual dictionary. The food images are represented as visual words distributions (Bag of Textons) and a Support Vector Machine is used for the classification stage. The experiments demonstrate that the image representation based on Bag of Textons is more accurate than existing (and more complex) approaches in classifying the 61 classes of the Pittsburgh Fast-Food Image Dataset.},
author = {Farinella, Giovanni Maria and Moltisanti, Marco and Battiato, Sebastiano},
booktitle = {2014 IEEE International Conference on Image Processing, ICIP 2014},
doi = {10.1109/ICIP.2014.7026055},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Farinella, Moltisanti, Battiato{\_}2014 IEEE International Conference on Image Processing, ICIP 2014(2).pdf:pdf},
isbn = {9781479957514},
keywords = {Bag of Words,Food Classification,Textons},
month = {oct},
pages = {5212--5216},
publisher = {IEEE},
title = {{Classifying food images represented as Bag of Textons}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7026055},
year = {2014}
}
@article{Burke2011a,
abstract = {Self-monitoring is the centerpiece of behavioral weight loss intervention programs. This article presents a systematic review of the literature on three components of self-monitoring in behavioral weight loss studies: diet, exercise, and self-weighing. This review included articles that were published between 1993 and 2009 that reported on the relationship between weight loss and these self-monitoring strategies. Of the 22 studies identified, 15 focused on dietary self-monitoring, one on self-monitoring exercise, and six on self-weighing. A wide array of methods was used to perform self-monitoring; the paper diary was used most often. Adherence to self-monitoring was reported most frequently as the number of diaries completed or the frequency of log-ins or reported weights. The use of technology, which included the Internet, personal digital assistants, and electronic digital scales were reported in five studies. Descriptive designs were used in the earlier studies whereas more recent reports involved prospective studies and randomized trials that examined the effect of self-monitoring on weight loss. A significant association between self-monitoring and weight loss was consistently found; however, the level of evidence was weak because of methodologic limitations. The most significant limitations of the reviewed studies were the homogenous samples and reliance on self-report. In all but two studies, the samples were predominantly white and women. This review highlights the need for studies in more diverse populations, for objective measures of adherence to self-monitoring, and for studies that establish the required dose of self-monitoring for successful outcomes. ?? 2011 American Dietetic Association.},
author = {Burke, Lora E. and Wang, Jing and Sevick, Mary Ann},
doi = {10.1016/j.jada.2010.10.008},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Burke, Wang, Sevick{\_}Journal of the American Dietetic Association.pdf:pdf},
isbn = {1878-3570 (Electronic)$\backslash$n0002-8223 (Linking)},
issn = {00028223},
journal = {Journal of the American Dietetic Association},
number = {1},
pages = {92--102},
pmid = {21185970},
publisher = {Elsevier Inc.},
title = {{Self-Monitoring in Weight Loss: A Systematic Review of the Literature}},
url = {http://dx.doi.org/10.1016/j.jada.2010.10.008},
volume = {111},
year = {2011}
}
@article{Kawano2015,
abstract = {In this paper, we propose a novel effective framework to ex-pand an existing image dataset automatically leveraging existing cat-egories and crowdsourcing. Especially, in this paper, we focus on ex-pansion on food image data set. The number of food categories is un-countable, since foods are different from a place to a place. If we have a Japanese food dataset, it does not help build a French food recognition system directly. That is why food data sets for different food cultures have been built independently category so far. Then, in this paper, we propose to leverage existing knowledge on food of other cultures by a generic " foodness " classifier and domain adaptation. This can enable us not only to built other-cultured food datasets based on an original food image dataset automatically, but also to save as much crowd-sourcing costs as possible. In the experiments, we show the effectiveness of the proposed method over the baselines.},
author = {Kawano, Yoshiyuki and Yanai, Keiji},
doi = {10.1007/978-3-319-16199-0_1},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Kawano, Yanai{\_}Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioin.pdf:pdf},
isbn = {9783319161983},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Adaptive SVM,Crowd-sourcing,Dataset expansion,Domain adaptation,Food image,Foodness},
pages = {3--17},
title = {{Automatic expansion of a food image dataset leveraging existing categories with domain adaptation}},
volume = {8927},
year = {2015}
}
@article{Zhu2015,
abstract = {We propose a method for dietary assessment to automatically identify and locate food in a variety of images captured during controlled and natural eating events. Two concepts are combined to achieve this: a set of segmented objects can be partitioned into perceptually similar object classes based on global and local features; and perceptually similar object classes can be used to assess the accuracy of image segmentation. These ideas are implemented by generating multiple segmentations of an image to select stable segmentations based on the classifier's confidence score assigned to each segmented image region. Automatic segmented regions are classified using a multichannel feature classification system. For each segmented region, multiple feature spaces are formed. Feature vectors in each of the feature spaces are individually classified. The final decision is obtained by combining class decisions from individual feature spaces using decision rules. We show improved accuracy of segmenting food images with classifier feedback.},
author = {Zhu, Fengqing and Bosch, Marc and Khanna, Nitin and Boushey, Carol J. and Delp, Edward J.},
doi = {10.1109/JBHI.2014.2304925},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Zhu et al.{\_}IEEE Journal of Biomedical and Health Informatics(2).pdf:pdf},
issn = {21682194},
journal = {IEEE Journal of Biomedical and Health Informatics},
keywords = {Dietary assessment,image analysis,image features,image segmentation,object recognition},
month = {jan},
number = {1},
pages = {377--388},
pmid = {25561457},
publisher = {IEEE},
title = {{Multiple hypotheses image segmentation and classification with application to dietary assessment}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6733271},
volume = {19},
year = {2015}
}
@inproceedings{Zong2010,
abstract = {This paper proposes a food image classification method using local textural patterns and their global structure to describe the food image. In this paper, a visual codebook of local textural patterns is created by employing Scale Invariant Feature Transformation (SIFT) interest point detector with the Local Binary Pattern (LBP) feature. In addition to describing the food image using local texture, the global structure of the food object is represented as the spatial distribution of the local textural structures and encoded using shape context. We evaluated the proposed method on the Pittsburgh Fast-Food Image (PFI) dataset. Experimental results showed that the proposed method could obtain better performance than the baseline experiment on the PFI dataset.},
author = {Zong, Zhimin and Nguyen, Duc Thanh and Ogunbona, Philip and Li, Wanqing},
booktitle = {Proceedings - 2010 IEEE International Symposium on Multimedia, ISM 2010},
doi = {10.1109/ISM.2010.37},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Zong et al.{\_}Proceedings - 2010 IEEE International Symposium on Multimedia, ISM 2010(2).pdf:pdf},
isbn = {9780769542171},
keywords = {Food classification,Local binary pattern,Shape context},
month = {dec},
pages = {204--211},
publisher = {IEEE},
title = {{On the combination of local texture and global structure for food classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5693842},
year = {2010}
}
@article{Arandjelovic2012,
abstract = {The objective of this work is object retrieval in large scale image datasets, where the object is specified by an image query and retrieval should be immediate at run time in the manner of Video Google [28]. We make the following three contributions: (i) a new method to compare SIFT descriptors (RootSIFT) which yields superior performance without increasing process- ing or storage requirements; (ii) a novel method for query expansion where a richer model for the query is learnt discriminatively in a form suited to immediate retrieval through efficient use of the inverted index; (iii) an improve- ment of the image augmentationmethod proposed by Turcot and Lowe [29], where only the augmenting features which are spatially consistent with the augmented image are kept. We evaluate these three methods over a number of stan- dard benchmark datasets (Oxford Buildings 5k and 105k, and Paris 6k) and demonstrate substantial improvements in retrieval performance whilst maintaining immediate re- trieval speeds. Combining these complementary meth- ods achieves a new state-of-the-art performance on these datasets.},
author = {Arandjelovic, Relja and Zisserman, Andrew},
doi = {10.1109/CVPR.2012.6248018},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Arandjelovic, Zisserman{\_}IEEE Conference on computer vision and Pattern Recognition.pdf:pdf},
isbn = {9781467312288},
issn = {9781467312288},
journal = {IEEE Conference on computer vision and Pattern Recognition},
number = {April},
pages = {2911--2918},
title = {{Three things everyone should know to improve object retrieval c}},
year = {2012}
}
@inproceedings{Yanai2015,
author = {Yanai, Keiji and Kawano, Yoshiyuki},
booktitle = {2015 IEEE International Conference on Multimedia {\&} Expo Workshops (ICMEW)},
doi = {10.1109/ICMEW.2015.7169816},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Yanai, Kawano{\_}2015 IEEE International Conference on Multimedia {\&} Expo Workshops (ICMEW).pdf:pdf},
isbn = {978-1-4799-7079-7},
month = {jun},
pages = {1--6},
publisher = {IEEE},
title = {{Food image recognition using deep convolutional network with pre-training and fine-tuning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7169816},
year = {2015}
}
@article{Bolanos2016,
abstract = {The development of automatic nutrition diaries, which would allow to keep track objectively of everything we eat, could enable a whole new world of possibilities for people concerned about their nutrition patterns. With this purpose, in this paper we propose the first method for simultaneous food localization and recognition. Our method is based on two main steps, which consist in, first, produce a food activation map on the input image (i.e. heat map of probabilities) for generating bounding boxes proposals and, second, recognize each of the food types or food-related objects present in each bounding box. We demonstrate that our proposal, compared to the most similar problem nowadays - object localization, is able to obtain high precision and reasonable recall levels with only a few bounding boxes. Furthermore, we show that it is applicable to both conventional and egocentric images.},
archivePrefix = {arXiv},
arxivId = {1604.07953},
author = {Bola{\~{n}}os, Marc and Radeva, Petia},
eprint = {1604.07953},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Bola{\~{n}}os, Radeva{\_}Unknown.pdf:pdf},
pages = {2--7},
title = {{Simultaneous Food Localization and Recognition}},
url = {http://arxiv.org/abs/1604.07953},
year = {2016}
}
@inproceedings{Lazebnik2006,
abstract = { This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting "spatial pyramid" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba{\&}{\#}146;s "gist" and Lowe{\&}{\#}146;s SIFT descriptors.},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Lazebnik, Svetlana and Schmid, Cordelia and Ponce, Jean},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2006.68},
eprint = {9411012},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Lazebnik, Schmid, Ponce{\_}Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition(2).pdf:pdf},
isbn = {0769525970},
issn = {10636919},
keywords = {Histograms,Image databases,Image recognition,Image representation,Image segmentation,Layout,Object recognition,Robustness,Shape,Spatial databases},
pages = {2169--2178},
primaryClass = {chao-dyn},
publisher = {IEEE},
title = {{Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1641019},
volume = {2},
year = {2006}
}
@article{TaichiJoutou2009,
abstract = {Since health care on foods is drawing people's attention recently, a system that can record everyday meals easily is being awaited. In this paper, we propose an automatic food image recognition system for recording people's eating habits. In the proposed system, we use the Multiple Kernel Learning (MKL) method to integrate several kinds of image features such as color, texture and SIFT adaptively. MKL enables to estimate optimal weights to combine image features for each category. In addition, we implemented a prototype system to recognize food images taken by cellular-phone cameras. In the experiment, we have achieved the 61.34{\%} classification rate for 50 kinds of foods. To the best of our knowledge, this is the first report of a food image classification system which can be applied for practical use.},
author = {{Taichi Joutou} and {Keiji Yanai}},
doi = {10.1109/ICIP.2009.5413400},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Taichi Joutou, Keiji Yanai{\_}2009 16th IEEE International Conference on Image Processing (ICIP)(2).pdf:pdf},
isbn = {978-1-4244-5653-6},
issn = {9781424456543},
journal = {2009 16th IEEE International Conference on Image Processing (ICIP)},
keywords = {food image,generic object recognition,multiple kernel learning},
month = {nov},
pages = {285--288},
publisher = {IEEE},
title = {{A food image recognition system with Multiple Kernel Learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5413400},
year = {2009}
}
@article{Zhu2011,
abstract = {Given a dataset of images, we seek to automatically identify and locate perceptually similar objects. We combine two ideas to achieve this: a set of segmented objects can be partitioned into perceptually similar object classes based on global and local features; and perceptually similar object classes can be used to assess the accuracy of image segmentation. These ideas are implemented by generating multiple segmentations of each image and then learning the object class by combining different segmentations to generate optimal segmentation. We demonstrate that the proposed method can be used as part of a new dietary assessment tool to automatically identify and locate the foods in a variety of food images captured during different user studies.},
author = {Zhu, Fengqing and Bosch, Marc and Khanna, Nitin and Boushey, Carol J. and Delp, Edward J.},
file = {:home/nogaret/Desktop/cranfield/thesis/pdf/Zhu et al.{\_}2011 7th International Symposium on Image and Signal Processing and Analysis (ISPA)(2).pdf:pdf},
isbn = {978-1-4577-0841-1},
issn = {1845-5921},
journal = {2011 7th International Symposium on Image and Signal Processing and Analysis (ISPA)},
number = {Ispa},
pages = {337--342},
pmid = {22127051},
title = {{Multilevel segmentation for food classification in dietary assessment}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6046629},
year = {2011}
}
